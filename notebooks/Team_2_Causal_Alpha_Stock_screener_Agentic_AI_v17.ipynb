{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDZ7StP_4qt5"
      },
      "source": [
        "# StockIQ - Agentic Screener for AI based Outlook & Recommendation\n",
        "\n",
        "This notebook provides a fully functional implementation of the Stock Outlook & Recommendation tool.  \n",
        "Each section is modularized into separate cells with explanatory markdown above the code cells.  \n",
        "Follow along to understand how the tool is configured, how it fetches and caches data, how it analyzes stocks, and how it presents results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bovqbOPwmI3r"
      },
      "source": [
        "## System Components:\n",
        "1. **Macro Analyst Agent**: Analyzes Indian macroeconomic factors\n",
        "2. **Sector and Company Analyst Agent**: Examines industry trends and company fundamentals\n",
        "3. **Technical Analyst Agent**: Analyzes price action and technical indicators\n",
        "4. **Orchestrator**: Coordinates the sub-agents and manages the analysis flow\n",
        "5. **Final Synthesis Agent**: Weighs all inputs and produces the final recommendation\n",
        "\n",
        "The system includes a soft cache to store previously retrieved data with timestamps to minimize unnecessary API calls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmXED3uF4qt7"
      },
      "source": [
        "## 1. Imports & Configuration\n",
        "In this cell, we import all necessary libraries and set up environment variables and configuration constants."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gradio==3.38.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GHtw5XaF6hiO",
        "outputId": "43874d7c-c2f7-4d40-c22c-5884eb030b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio==3.38.0 in /usr/local/lib/python3.11/dist-packages (3.38.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (23.2.1)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (3.11.15)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (0.5.0)\n",
            "Requirement already satisfied: gradio-client>=0.2.10 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (1.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (3.1.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (2.2.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (3.10.0)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (0.3.3)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (3.10.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (10.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (6.0.2)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (2.32.3)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (0.34.2)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.38.0) (11.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (1.20.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==3.38.0) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==3.38.0) (1.37.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client>=0.2.10->gradio==3.38.0) (2025.3.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.38.0) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.38.0) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.38.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.38.0) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->gradio==3.38.0) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.14.0->gradio==3.38.0) (3.18.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.14.0->gradio==3.38.0) (4.67.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.0.0->markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (2.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.38.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.38.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.38.0) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.38.0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.38.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.38.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==3.38.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==3.38.0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.38.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.38.0) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.38.0) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.0->gradio==3.38.0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.0->gradio==3.38.0) (2.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.14.0->gradio==3.38.0) (8.1.8)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi->gradio==3.38.0) (0.46.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0) (0.24.0)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (1.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.38.0) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->gradio==3.38.0) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports_and_config"
      },
      "execution_count": null,
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, timedelta, date\n",
        "import time\n",
        "from openai import OpenAI\n",
        "import pathlib\n",
        "import sys\n",
        "import locale\n",
        "import re\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import hashlib\n",
        "from itertools import combinations\n",
        "import collections\n",
        "\n",
        "# ===== Configuration Parameters =====\n",
        "CACHE_FRESHNESS_HOURS = 1000  # hours before price cache is stale\n",
        "CACHE_FILE_PATH = \"cache.json\"\n",
        "PRICE_CACHE_PATH = \"price_cache.json\"\n",
        "MODEL_NAME = \"gpt-4o\"\n",
        "DEFAULT_LOOKBACK_YEARS = 3\n",
        "ROLLING_WINDOW_DAYS = 252\n",
        "\n",
        "from google.colab import userdata\n",
        "openai_key = userdata.get('OPENAI_API_KEY')\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", openai_key)\n",
        "client = OpenAI()\n",
        "\n",
        "# Force UTF-8\n",
        "os.environ[\"PYTHONIOENCODING\"] = \"utf-8\"\n",
        "\n",
        "# Load embedding model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of reliable/trusted sites which the Agents can rely on - to do web research\n",
        "trusted_urls_macro = [\n",
        "    \"rbi.org.in/\",\n",
        "    \"finmin.gov.in/\",\n",
        "    \"mospi.gov.in/\",\n",
        "    \"sebi.gov.in/\",\n",
        "    \"nseindia.com\",\n",
        "    \"bseindia.com\",\n",
        "    \"data.worldbank.org/\",\n",
        "    \"data.oecd.org/\",\n",
        "    \"tradingeconomics.com/\",\n",
        "    \"fred.stlouisfed.org/\",\n",
        "    \"ceicdata.com/\",\n",
        "    \"investing.com/\",  # /commodities/brent-oil\n",
        "    \"tradingeconomics.com/\",  # india/currency\n",
        "    \"imf.org/\",  # en/Publications/WEO\n",
        "    \"rsisinternational.org/\",\n",
        "    \"brokerage-free.in/\",\n",
        "    \"commerce.gov.in/\",  # Export-import data by sector\n",
        "    \"finmin.nic.in/\",  # Budget allocations by sector\n",
        "    \"indiabudget.gov.in/economicsurvey/\",  # Macro + Sector performance annually\n",
        "    \"data.worldbank.org/country/india\",  # Sectoral development data (infra, finance, health)\n",
        "    \"cmie.com/\",  # Premium sectoral data\n",
        "]\n",
        "\n",
        "trusted_urls_sector = [\n",
        "    \"wikipedia.org/\",\n",
        "    \"reuters.com/\",\n",
        "    \"ticker.finology.in/\",\n",
        "    \"ft.com/\",\n",
        "    \"ibef.org/industry\",  # Official industry and sector reports (India-specific)\n",
        "    \"moneycontrol.com/\",\n",
        "    \"statista.com/markets/\",  # Global sector data, statistics, industry trends\n",
        "    \"mckinsey.com/industries\",  # Deep strategic sector reports (global + India)\n",
        "    \"www2.deloitte.com/\",  # /global/en/insights/industry.html - Research on financial services, energy, consumer goods\n",
        "    \"home.kpmg/\",  # /xx/en/home/insights.html - Sector-specific trends and analyses\n",
        "    \"rbi.org.in/\",  # /Scripts/Publications.aspx?head=Reports - Sectoral credit growth, sector stress analysis\n",
        "    \"sebi.gov.in/\",  # /sebiweb/home/HomeAction.do?doListing=yes - Financial sector outlook, regulatory updates\n",
        "    \"nseindia.com/\",  # /market-data/live-equity-market - Sector index performance (Auto, FMCG, Pharma, etc.)\n",
        "    \"reuters.com/business/\",  # Global sector news\n",
        "    \"bloomberg.com/markets/sectors\",  # Premium sector analysis\n",
        "    \"ft.com/companies\",  # Comprehensive sector coverage\n",
        "    \"bseindia.com/markets/Indices/Indices.aspx?expandable=0\",  # Sector-wise performance metrics\n",
        "    \"finance.yahoo.com/sectors/\",  # Broad sector movements (Energy, Tech, Financials)\n",
        "    \"tradingeconomics.com/india/indicators\",  # Industry statistics (energy, services, agriculture)\n",
        "    \"economictimes.indiatimes.com/industry\",  # Latest sector-specific news for Indian industries\n",
        "    \"business-standard.com/industry\",  # Indian company & sector updates\n",
        "    \"cnbc.com/sectors/\",  # Sector analysis for US, EU, Asia\n",
        "    \"stats.oecd.org/\",  # Global sector data (manufacturing, finance, services)\n",
        "    \"data.imf.org/\",  # Financial health of sectors globally\n",
        "    \"morningstar.in/default.aspx\",\n",
        "    \"in.investing.com/indices/major-indices\",\n",
        "    \"web.stockedge.com/sectors\",\n",
        "    \"tradingview.com/markets/stocks-india/sectorandindustry-sector/\",\n",
        "]\n",
        "\n",
        "trusted_urls_technical = [\n",
        "    \"nseindia.com\",\n",
        "    \"bseindia.com\",\n",
        "    \"moneycontrol.com\",\n",
        "    # \"twelvedata.com\",\n",
        "    # \"alphavantage.co\",\n",
        "    # \"upstox.com/\",\n",
        "    \"finance.yahoo.com\",\n",
        "    # \"tradingview.com\",\n",
        "    \"in.investing.com/\",\n",
        "    \"investtech.com/\",\n",
        "    \"finviz.com\",\n",
        "    # \"chartink.com/\",\n",
        "    # \"stockcharts.com\",\n",
        "    # \"marketsmithindia.com/\",\n",
        "    \"barchart.com\",\n",
        "    \"web.stockedge.com\",\n",
        "    # \"trendlyne.com\",\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# 1. Collapse each list into a comma-separated string\n",
        "trusted_urls_macro_str = \", \".join(trusted_urls_macro)\n",
        "trusted_urls_sector_str = \", \".join(trusted_urls_sector)\n",
        "trusted_urls_technical_str = \", \".join(trusted_urls_technical)\n",
        "\n",
        "# 2. Guardrail template\n",
        "GUARDRAIL_TEMPLATE = (\n",
        "    \"Use only these trusted sources for factual data: {sources_list}. \"\n",
        "    \"Rely exclusively on them when conducting this analysis.\"\n",
        ")\n",
        "\n",
        "# 3. Toggle for experimental guardrail usage\n",
        "USE_TRUSTED_SOURCES = False  # default OFF; set True to enable guardrail in prompts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Example: iterating over the list\n",
        "for site in trusted_urls[0:3]:\n",
        "    print(f\"Accessing data from: {site}\")\n",
        "\n",
        "# Undoing all usage of this variable in LLM calls - need to experiment with other architectures\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "-5p8IAIaQuNy",
        "outputId": "019705ba-921e-45fd-cafc-7acdac5068a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Example: iterating over the list\\nfor site in trusted_urls[0:3]:\\n    print(f\"Accessing data from: {site}\")\\n\\n# Undoing all usage of this variable in LLM calls - need to experiment with other architectures\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dkOLJm34qt8"
      },
      "source": [
        "## 2. Price Cache Utilities\n",
        "Functions to load, save, and check freshness of the price cache."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "price_cache_utils"
      },
      "execution_count": null,
      "source": [
        "def load_price_cache() -> Dict:\n",
        "    try:\n",
        "        if pathlib.Path(PRICE_CACHE_PATH).exists():\n",
        "            return json.load(open(PRICE_CACHE_PATH))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading price cache: {e}\")\n",
        "    return {}\n",
        "#\n",
        "def save_price_cache(cache: Dict):\n",
        "    try:\n",
        "        json.dump(cache, open(PRICE_CACHE_PATH, 'w'), indent=2)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving price cache: {e}\")\n",
        "\n",
        "def is_price_cache_fresh(ticker: str, freshness_hours: int = CACHE_FRESHNESS_HOURS) -> bool:\n",
        "    cache = load_price_cache()\n",
        "    entry = cache.get(ticker, {})\n",
        "    ts = entry.get(\"last_updated\")\n",
        "    if not ts:\n",
        "        return False\n",
        "    try:\n",
        "        last = datetime.fromisoformat(ts)\n",
        "    except:\n",
        "        return False\n",
        "    return (datetime.utcnow() - last) < timedelta(hours=freshness_hours)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmu1yQXW4qt9"
      },
      "source": [
        "## 3. Fetching Historical Data & Indicators\n",
        "Function to download stock and benchmark data, compute returns, moving averages, Bollinger Bands, RSI, MACD, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "get_stock_data"
      },
      "execution_count": null,
      "source": [
        "def get_stock_data(ticker: str, start_date=None, end_date=None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    # Set default dates\n",
        "    if end_date is None:\n",
        "        end_date = date.today()\n",
        "    if start_date is None:\n",
        "        start_date = end_date - timedelta(days=365 * DEFAULT_LOOKBACK_YEARS)\n",
        "\n",
        "    # Download stock data\n",
        "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    if stock_data.empty:\n",
        "        raise ValueError(f\"No data for ticker {ticker}\")\n",
        "    if isinstance(stock_data.columns, pd.MultiIndex):\n",
        "        stock_data.columns = stock_data.columns.get_level_values(0)\n",
        "\n",
        "    # Download benchmark data\n",
        "    nifty_data = yf.download(\"^NSEI\", start=start_date, end=end_date)\n",
        "    if isinstance(nifty_data.columns, pd.MultiIndex):\n",
        "        nifty_data.columns = nifty_data.columns.get_level_values(0)\n",
        "\n",
        "    # Align indices\n",
        "    nifty_data = nifty_data.reindex(stock_data.index, method='ffill')\n",
        "\n",
        "    # Calculate daily returns\n",
        "    stock_data['Daily_Return'] = stock_data['Close'].pct_change()\n",
        "    nifty_data['Daily_Return'] = nifty_data['Close'].pct_change()\n",
        "\n",
        "    # Relative performance\n",
        "    stock_data['Rel_Performance'] = stock_data['Daily_Return'] - nifty_data['Daily_Return']\n",
        "    stock_data['Cum_Rel_Performance'] = (1 + stock_data['Rel_Performance']).cumprod() - 1\n",
        "\n",
        "    # Volatility (returns std)\n",
        "    stock_data['Volatility'] = stock_data['Daily_Return'].rolling(window=20).std()\n",
        "\n",
        "    # Moving averages\n",
        "    stock_data['MA_20'] = stock_data['Close'].rolling(window=20).mean()\n",
        "    stock_data['MA_50'] = stock_data['Close'].rolling(window=50).mean()\n",
        "    stock_data['MA_200'] = stock_data['Close'].rolling(window=200).mean()\n",
        "\n",
        "    # Bollinger Bands (price std)\n",
        "    price_std = stock_data['Close'].rolling(window=20).std()\n",
        "    stock_data['Upper_Band'] = stock_data['MA_20'] + 2 * price_std\n",
        "    stock_data['Lower_Band'] = stock_data['MA_20'] - 2 * price_std\n",
        "\n",
        "    # RSI\n",
        "    delta = stock_data['Close'].diff()\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "    avg_gain = gain.rolling(window=14).mean()\n",
        "    avg_loss = loss.rolling(window=14).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    stock_data['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # MACD\n",
        "    stock_data['EMA_12'] = stock_data['Close'].ewm(span=12, adjust=False).mean()\n",
        "    stock_data['EMA_26'] = stock_data['Close'].ewm(span=26, adjust=False).mean()\n",
        "    stock_data['MACD'] = stock_data['EMA_12'] - stock_data['EMA_26']\n",
        "    stock_data['MACD_Signal'] = stock_data['MACD'].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    return stock_data, nifty_data"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIOnctVy4qt9"
      },
      "source": [
        "## 4. Serialization of DataFrame\n",
        "Convert a DataFrame of indicators into a JSON-serializable dictionary with summary metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "serialize_dataframe"
      },
      "execution_count": null,
      "source": [
        "def serialize_dataframe(df: pd.DataFrame) -> Dict:\n",
        "    \"\"\"Convert DataFrame to JSON-serializable dict including indicators and summary metrics.\"\"\"\n",
        "    n = len(df)\n",
        "    last_date = df.index[-1]\n",
        "    current_price = float(df['Close'].iloc[-1])\n",
        "    # Year-to-date return\n",
        "    try:\n",
        "        start_of_year = datetime(last_date.year, 1, 1)\n",
        "        ytd_slice = df.loc[df.index >= start_of_year, 'Close']\n",
        "        if not ytd_slice.empty:\n",
        "            ytd_start = float(ytd_slice.iloc[0])\n",
        "            ytd_return = (current_price / ytd_start - 1) * 100\n",
        "        else:\n",
        "            ytd_return = 0.0\n",
        "    except Exception:\n",
        "        ytd_return = 0.0\n",
        "    # Multi-year returns annualized\n",
        "    return_1y = return_3y = return_5y = 0.0\n",
        "    days = n\n",
        "    if days > ROLLING_WINDOW_DAYS:\n",
        "        price_1y = float(df['Close'].iloc[-ROLLING_WINDOW_DAYS])\n",
        "        return_1y = (current_price / price_1y - 1) * 100\n",
        "    if days > ROLLING_WINDOW_DAYS * 3:\n",
        "        price_3y = float(df['Close'].iloc[-ROLLING_WINDOW_DAYS * 3])\n",
        "        return_3y = ((current_price / price_3y) ** (1/3) - 1) * 100\n",
        "    if days > ROLLING_WINDOW_DAYS * 5:\n",
        "        price_5y = float(df['Close'].iloc[-ROLLING_WINDOW_DAYS * 5])\n",
        "        return_5y = ((current_price / price_5y) ** (1/5) - 1) * 100\n",
        "    current_rsi = float(df['RSI'].iloc[-1]) if 'RSI' in df else 0.0\n",
        "    current_volatility = float(df['Volatility'].iloc[-1] * 100) if 'Volatility' in df else 0.0\n",
        "\n",
        "    return {\n",
        "        'dates': df.index.strftime('%Y-%m-%d').tolist(),\n",
        "        'open': df['Open'].tolist(),\n",
        "        'high': df['High'].tolist(),\n",
        "        'low': df['Low'].tolist(),\n",
        "        'close': df['Close'].tolist(),\n",
        "        'volume': df['Volume'].tolist() if 'Volume' in df.columns else [0]*n,\n",
        "        'indicators': {\n",
        "            'daily_return': df['Daily_Return'].fillna(0).tolist(),\n",
        "            'volatility': df['Volatility'].fillna(0).tolist(),\n",
        "            'ma_20': df['MA_20'].fillna(0).tolist(),\n",
        "            'ma_50': df['MA_50'].fillna(0).tolist(),\n",
        "            'ma_200': df['MA_200'].fillna(0).tolist(),\n",
        "            'rsi': df['RSI'].fillna(0).tolist(),\n",
        "            'macd': df['MACD'].fillna(0).tolist(),\n",
        "            'upper_band': df['Upper_Band'].fillna(0).tolist(),\n",
        "            'lower_band': df['Lower_Band'].fillna(0).tolist(),\n",
        "        },\n",
        "        'summary_metrics': {\n",
        "            'current_price': current_price,\n",
        "            'ytd_return': ytd_return,\n",
        "            'return_1y': return_1y,\n",
        "            'return_3y': return_3y,\n",
        "            'return_5y': return_5y,\n",
        "            'current_rsi': current_rsi,\n",
        "            'current_volatility': current_volatility\n",
        "        }\n",
        "    }"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpJ5psft4qt-"
      },
      "source": [
        "## 5. Cache-Backed Fetch Function\n",
        "Fetch price data with cache support, falling back to live fetch if cache is stale or force_refresh=True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "get_ticker_price_data"
      },
      "execution_count": null,
      "source": [
        "def get_ticker_price_data(\n",
        "    ticker: str,\n",
        "    start_date=None,\n",
        "    end_date=None,\n",
        "    force_refresh: bool = False\n",
        ") -> Tuple[pd.DataFrame, Dict, pd.DataFrame]:\n",
        "    ticker = ticker.upper()\n",
        "    cache = load_price_cache()\n",
        "\n",
        "    if not force_refresh and is_price_cache_fresh(ticker):\n",
        "        entry = cache[ticker]['price_history']\n",
        "        dates = pd.to_datetime(entry['dates'])\n",
        "        df = pd.DataFrame({\n",
        "            'Open': entry['open'],\n",
        "            'High': entry['high'],\n",
        "            'Low': entry['low'],\n",
        "            'Close': entry['close'],\n",
        "            'Volume': entry['volume'],\n",
        "            'Daily_Return': entry['indicators']['daily_return'],\n",
        "            'Volatility': entry['indicators']['volatility'],\n",
        "            'MA_20': entry['indicators']['ma_20'],\n",
        "            'MA_50': entry['indicators']['ma_50'],\n",
        "            'MA_200': entry['indicators']['ma_200'],\n",
        "            'RSI': entry['indicators']['rsi'],\n",
        "            'MACD': entry['indicators']['macd'],\n",
        "            'Upper_Band': entry['indicators']['upper_band'],\n",
        "            'Lower_Band': entry['indicators']['lower_band'],\n",
        "        }, index=dates)\n",
        "        nifty = yf.download(\"^NSEI\", start=dates[0], end=dates[-1] + timedelta(days=1))\n",
        "        nifty = nifty.reindex(df.index, method='ffill')\n",
        "        return df, entry, nifty\n",
        "\n",
        "    # Fetch fresh\n",
        "    df, nifty = get_stock_data(ticker, start_date, end_date)\n",
        "    history = serialize_dataframe(df)\n",
        "    cache.setdefault(ticker, {})['price_history'] = history\n",
        "    cache[ticker]['last_updated'] = datetime.utcnow().isoformat()\n",
        "    save_price_cache(cache)\n",
        "    return df, history, nifty"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9DFHIKO4qt-"
      },
      "source": [
        "## 6. Plotting Functions\n",
        "Functions to plot price, moving averages, Bollinger Bands, RSI, volatility, MACD with Plotly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plot_price_charts"
      },
      "execution_count": null,
      "source": [
        "def plot_price_charts(stock_data: pd.DataFrame, ticker: str, nifty_data: Optional[pd.DataFrame]=None, benchmark: str=\"^NSEI\"):\n",
        "    # Price + MAs + benchmark\n",
        "    fig1 = go.Figure()\n",
        "    fig1.add_trace(go.Scatter(x=stock_data.index, y=stock_data['Close'], mode='lines', name=f'{ticker} Close'))\n",
        "    fig1.add_trace(go.Scatter(x=stock_data.index, y=stock_data['MA_50'], mode='lines', name='50-day MA', line=dict(dash='dash')))\n",
        "    fig1.add_trace(go.Scatter(x=stock_data.index, y=stock_data['MA_200'], mode='lines', name='200-day MA', line=dict(dash='dash')))\n",
        "    if nifty_data is not None and not nifty_data.empty:\n",
        "        sf = stock_data['Close'].iloc[0] / nifty_data['Close'].iloc[0]\n",
        "        fig1.add_trace(go.Scatter(x=nifty_data.index, y=nifty_data['Close']*sf, mode='lines', name=f'{benchmark} (scaled)'))\n",
        "    fig1.update_layout(title=f'{ticker} Price & MAs', xaxis_title='Date', yaxis_title='Price')\n",
        "    # fig1.show()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    fig2 = go.Figure()\n",
        "    fig2.add_trace(go.Scatter(x=stock_data.index, y=stock_data['Close'], mode='lines', name='Close'))\n",
        "    fig2.add_trace(go.Scatter(x=stock_data.index, y=stock_data['Upper_Band'], mode='lines', name='Upper Band', line=dict(width=0.5)))\n",
        "    fig2.add_trace(go.Scatter(x=stock_data.index, y=stock_data['Lower_Band'], mode='lines', name='Lower Band', fill='tonexty', line=dict(width=0.5)))\n",
        "    fig2.update_layout(title=f'{ticker} Bollinger Bands', xaxis_title='Date', yaxis_title='Price')\n",
        "    # fig2.show()\n",
        "\n",
        "    # RSI\n",
        "    fig3 = go.Figure()\n",
        "    fig3.add_trace(go.Scatter(x=stock_data.index, y=stock_data['RSI'], mode='lines', name='RSI'))\n",
        "    fig3.add_hline(y=70, line_dash='dash', annotation_text='Overbought')\n",
        "    fig3.add_hline(y=30, line_dash='dash', annotation_text='Oversold')\n",
        "    fig3.update_layout(title=f'{ticker} RSI', xaxis_title='Date', yaxis_title='RSI')\n",
        "    fig3.update_yaxes(range=[0,100])\n",
        "    # fig3.show()\n",
        "\n",
        "    # Volatility\n",
        "    nifty_data['Daily_Return'] = nifty_data['Close'].pct_change()\n",
        "    fig4 = go.Figure()\n",
        "    fig4.add_trace(go.Scatter(x=stock_data.index, y=stock_data['Volatility']*100, mode='lines', name=f'{ticker} Volatility (%)'))\n",
        "    if nifty_data is not None and not nifty_data.empty:\n",
        "        nv = nifty_data['Daily_Return'].rolling(window=20).std()*100\n",
        "        fig4.add_trace(go.Scatter(x=nifty_data.index, y=nv, mode='lines', name=f'{benchmark} Volatility'))\n",
        "    fig4.update_layout(title=f'{ticker} vs {benchmark} Volatility', xaxis_title='Date', yaxis_title='Volatility (%)')\n",
        "    # fig4.show()\n",
        "\n",
        "    # MACD\n",
        "    # print(f'MACD : {stock_data.MACD_Signal}')\n",
        "    # fig5 = go.Figure()\n",
        "    # fig5.add_trace(go.Scatter(x=stock_data.index, y=stock_data['MACD'], mode='lines', name='MACD'))\n",
        "    # fig5.add_trace(go.Scatter(x=stock_data.index, y=stock_data['MACD_Signal'], mode='lines', name='Signal'))\n",
        "    # hist = stock_data['MACD'] - stock_data['MACD_Signal']\n",
        "    # colors = ['green' if v>=0 else 'red' for v in hist]\n",
        "    # fig5.add_trace(go.Bar(x=stock_data.index, y=hist, name='Histogram', marker_color=colors))\n",
        "    # fig5.update_layout(title=f'{ticker} MACD', xaxis_title='Date')\n",
        "    # fig5.show()\n",
        "\n",
        "    return {\n",
        "        \"price_chart\": fig1,\n",
        "        \"bollinger_chart\": fig2,\n",
        "        \"rsi_chart\": fig3,\n",
        "        \"volatility_chart\": fig4,\n",
        "        # \"macd_chart\": fig5\n",
        "    }"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGXBM96-4qt_"
      },
      "source": [
        "## 7. LLM Cache Implementation\n",
        "Functions to load, save, and validate freshness of the LLM result cache."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llm_cache"
      },
      "execution_count": null,
      "source": [
        "def load_cache():\n",
        "    \"\"\"Load the cache from the cache file or create a new one if it doesn't exist.\"\"\"\n",
        "    try:\n",
        "        if pathlib.Path(CACHE_FILE_PATH).exists():\n",
        "            with open(CACHE_FILE_PATH, 'r') as f:\n",
        "                return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading cache: {e}\")\n",
        "    return {}\n",
        "\n",
        "def save_cache(cache):\n",
        "    \"\"\"Save the cache to the cache file.\"\"\"\n",
        "    try:\n",
        "        with open(CACHE_FILE_PATH, 'w') as f:\n",
        "            json.dump(cache, f, indent=2)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving cache: {e}\")\n",
        "\n",
        "def is_cache_fresh(timestamp, freshness_hours=CACHE_FRESHNESS_HOURS):\n",
        "    \"\"\"Check if the cached data is fresh based on its timestamp.\"\"\"\n",
        "    if not timestamp:\n",
        "        return False\n",
        "\n",
        "    cache_time = datetime.fromisoformat(timestamp)\n",
        "    current_time = datetime.now()\n",
        "    return (current_time - cache_time) < timedelta(hours=freshness_hours)\n",
        "\n",
        "def get_cache_entry(ticker, domain):\n",
        "    \"\"\"Get a cache entry for a specific ticker and domain.\"\"\"\n",
        "    cache = load_cache()\n",
        "    if ticker not in cache:\n",
        "        return None\n",
        "    if domain not in cache[ticker]:\n",
        "        return None\n",
        "    entry = cache[ticker][domain]\n",
        "    if isinstance(entry.get(\"result\"), str) and \"error\" in entry.get(\"result\", \"\").lower():\n",
        "        return None\n",
        "    if is_cache_fresh(entry.get(\"last_updated\")):\n",
        "        return entry.get(\"result\")\n",
        "    return None\n",
        "\n",
        "def update_cache_entry(ticker, domain, result):\n",
        "    \"\"\"Update a cache entry for a specific ticker and domain.\"\"\"\n",
        "    cache = load_cache()\n",
        "    timestamp = datetime.now().isoformat()\n",
        "    if ticker not in cache:\n",
        "        cache[ticker] = {}\n",
        "    cache[ticker][domain] = {\n",
        "        \"result\": result,\n",
        "        \"last_updated\": timestamp\n",
        "    }\n",
        "    cache[ticker][\"ticker_last_updated\"] = timestamp\n",
        "    save_cache(cache)\n",
        "    return result"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omM2wtLI4qt_"
      },
      "source": [
        "##8. Utility Functions for LLM Interaction\n",
        "Text sanitization, prompt formatting, similarity computations, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llm_utils"
      },
      "execution_count": null,
      "source": [
        "def extract_text_from_response(response):\n",
        "    \"\"\"\n",
        "    Extract text content from the OpenAI Responses API response.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if hasattr(response, 'output') and isinstance(response.output, list):\n",
        "            for item in response.output:\n",
        "                if hasattr(item, 'type') and item.type == 'message':\n",
        "                    if hasattr(item, 'content') and isinstance(item.content, list):\n",
        "                        text_parts = []\n",
        "                        for content_item in item.content:\n",
        "                            if hasattr(content_item, 'text'):\n",
        "                                text_parts.append(content_item.text)\n",
        "                        if text_parts:\n",
        "                            return \"\\n\".join(text_parts)\n",
        "        if hasattr(response, 'model_dump'):\n",
        "            dump = response.model_dump()\n",
        "            if 'output' in dump and isinstance(dump['output'], list):\n",
        "                for item in dump['output']:\n",
        "                    if item.get('type') == 'message' and 'content' in item:\n",
        "                        for content in item['content']:\n",
        "                            if 'text' in content:\n",
        "                                return content['text']\n",
        "        return str(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from response: {e}\")\n",
        "        return str(response)\n",
        "\n",
        "def sanitize_utf8(text):\n",
        "    \"\"\"\n",
        "    Removes characters that are not valid UTF-8 and surrogate pairs that break serialization.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    text = re.sub(r'[\\uD800-\\uDFFF]', '', text)\n",
        "    text = ''.join(c for c in text if c.isprintable() or c in \"\\n\\t \")\n",
        "    return text\n",
        "\n",
        "def format_prompt_with_hallucination_control(task_prompt):\n",
        "    \"\"\"Add hallucination control instructions to a prompt.\"\"\"\n",
        "    return f\"{task_prompt}\\n\\nUse domain knowledge for standard definitions, but do not guess or invent real-time facts not present in the search results. If missing data, disclaim it: 'No data found.'\"\n",
        "\n",
        "def compute_similarity(text1, text2):\n",
        "    \"\"\"Compute cosine similarity between two text snippets.\"\"\"\n",
        "    emb1 = embedding_model.encode(text1, convert_to_tensor=True)\n",
        "    emb2 = embedding_model.encode(text2, convert_to_tensor=True)\n",
        "    return util.pytorch_cos_sim(emb1, emb2).item()\n",
        "\n",
        "def chunk_text(text, max_words=100):\n",
        "    \"\"\"Split text into chunks of max_words.\"\"\"\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i+max_words]) for i in range(0, len(words), max_words)]\n",
        "\n",
        "def average_cosine_similarity(base_output, comparison_output):\n",
        "    \"\"\"Compute average cosine similarity between two texts split into chunks.\"\"\"\n",
        "    base_chunks = chunk_text(base_output)\n",
        "    comp_chunks = chunk_text(comparison_output)\n",
        "    if not base_chunks or not comp_chunks:\n",
        "        return 0.0\n",
        "    sims = [compute_similarity(a, b) for a, b in zip(base_chunks, comp_chunks)]\n",
        "    return float(np.mean(sims))\n",
        "\n",
        "\n",
        "def strip_non_ascii(text):\n",
        "    \"\"\"Strip non-ASCII characters from text.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    return ''.join(ch for ch in text if ord(ch) < 128)\n",
        "\n",
        "def query_llm(prompt, tools=None):\n",
        "    \"\"\"\n",
        "    Query the OpenAI Responses API and return the cleaned text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Sanitize to valid UTF-8\n",
        "        prompt_clean = sanitize_utf8(prompt)\n",
        "        # (optionally strip non-ASCII if you want even stricter sanitization)\n",
        "        prompt_clean = strip_non_ascii(prompt_clean)\n",
        "\n",
        "        response = client.responses.create(\n",
        "            model=MODEL_NAME,\n",
        "            input=prompt_clean,\n",
        "            tools=tools or [],\n",
        "            temperature=0.2\n",
        "        )\n",
        "        return extract_text_from_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying LLM: {e}\")\n",
        "        return {\"error\": str(e)}\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def serialize_obj(o):\n",
        "    \"\"\"Custom serializer: use to_dict() if available, else str().\"\"\"\n",
        "    if hasattr(o, \"to_dict\"):\n",
        "        return o.to_dict()\n",
        "    return str(o)\n"
      ],
      "metadata": {
        "id": "vTlXuQle54er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n"
      ],
      "metadata": {
        "id": "wiPdQavB5eA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p36gZFyx4quA"
      },
      "source": [
        "## 9. BaseAgent Class Definition\n",
        "Implements methods to initialize agent, maintain memory, run sub-agents, and produce final synthesis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "base_agent"
      },
      "execution_count": null,
      "source": [
        "class BaseAgent:\n",
        "    def __init__(self, ticker, cache_freshness_hours=CACHE_FRESHNESS_HOURS):\n",
        "        self.ticker = ticker\n",
        "        self.memory = []  # Simple memory list for context\n",
        "        self.cache_freshness_hours = cache_freshness_hours\n",
        "        try:\n",
        "            stock_data, price_history, nifty_data = get_ticker_price_data(ticker)\n",
        "            self.price_data = price_history\n",
        "            self.add_to_memory(f\"Retrieved price data for {ticker} with {len(stock_data)} data points.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error retrieving price data for {ticker}: {e}\")\n",
        "            self.price_data = None\n",
        "            self.add_to_memory(f\"Failed to retrieve price data for {ticker}: {e}\")\n",
        "\n",
        "    def add_to_memory(self, text):\n",
        "        \"\"\"Add text to the agent's memory.\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            try:\n",
        "                text = json.dumps(text, default=lambda o: str(o))\n",
        "            except:\n",
        "                text = str(text)\n",
        "        self.memory.append(text)\n",
        "\n",
        "    def get_context(self):\n",
        "        \"\"\"Get the current context from memory.\"\"\"\n",
        "        return \"\\n\".join(self.memory[-5:])  # Last 5 messages for context\n",
        "\n",
        "    def run_sequence(self, tasks, weights=None):\n",
        "        \"\"\"Run a sequence of sub-agent tasks.\"\"\"\n",
        "        results = {}\n",
        "        for name, task_prompt in tasks.items():\n",
        "            domain = name.lower().replace(\" \", \"_\")\n",
        "            cached_result = get_cache_entry(self.ticker, domain)\n",
        "            if cached_result:\n",
        "                print(f\"Using cached data for {name} of {self.ticker}\")\n",
        "                results[name] = cached_result\n",
        "                self.add_to_memory(f\"{name} (Cached): {json.dumps(cached_result, indent=2)}\")\n",
        "                continue\n",
        "            print(f\"Fetching fresh data for {name} of {self.ticker}\")\n",
        "            price_context = \"\"\n",
        "            if self.price_data:\n",
        "                metrics = self.price_data.get('summary_metrics', {})\n",
        "                price_context = f\"\"\"\n",
        "                Price Data Context for {self.ticker}:\n",
        "                - Current Price: â‚¹{metrics.get('current_price', 'N/A')}\n",
        "                - YTD Return: {metrics.get('ytd_return', 'N/A'):.2f}%\n",
        "                - 1Y Return: {metrics.get('return_1y', 'N/A'):.2f}%\n",
        "                - 3Y Annualized Return: {metrics.get('return_3y', 'N/A'):.2f}%\n",
        "                - 5Y Annualized Return: {metrics.get('return_5y', 'N/A'):.2f}%\n",
        "                - Current RSI: {metrics.get('current_rsi', 'N/A'):.2f}\n",
        "                - Current Volatility: {metrics.get('current_volatility', 'N/A'):.2f}%\n",
        "                \"\"\"\n",
        "            context = self.get_context()\n",
        "            task_with_ticker = task_prompt.replace(\"[TICKER]\", self.ticker)\n",
        "            controlled_prompt = format_prompt_with_hallucination_control(task_with_ticker)\n",
        "            full_prompt = f\"Context:\\n{context}\\nPrice Data:\\n{price_context}\\nTask: {controlled_prompt}\"\n",
        "            result = query_llm(full_prompt, tools=[{\"type\": \"web_search\"}])\n",
        "            results[name] = result\n",
        "            self.add_to_memory(f\"{name}: {json.dumps(result, indent=2)}\")\n",
        "            update_cache_entry(self.ticker, domain, result)\n",
        "            time.sleep(1)\n",
        "        return results\n",
        "\n",
        "    def final_synthesis(self, sub_agent_outputs, weights=None):\n",
        "        \"\"\"\n",
        "        Generate a final synthesis based on sub-agent outputs.\n",
        "        \"\"\"\n",
        "        cached_result = get_cache_entry(self.ticker, \"final_synthesis\")\n",
        "        if cached_result:\n",
        "            print(f\"Using cached final synthesis for {self.ticker}\")\n",
        "            return cached_result\n",
        "        print(f\"Generating fresh final synthesis for {self.ticker}\")\n",
        "        outputs_json = json.dumps(sub_agent_outputs, indent=2, default=serialize_obj)\n",
        "        cache = load_cache()\n",
        "        timestamps = {}\n",
        "        if self.ticker in cache:\n",
        "            for domain in cache[self.ticker]:\n",
        "                if domain != \"ticker_last_updated\" and \"last_updated\" in cache[self.ticker][domain]:\n",
        "                    timestamps[domain] = cache[self.ticker][domain][\"last_updated\"]\n",
        "        timestamps_json = json.dumps(timestamps, indent=2)\n",
        "        price_context = \"\"\n",
        "        if self.price_data:\n",
        "            price_context = f\"\"\"\n",
        "            Historical Price Data Summary for {self.ticker}:\n",
        "            Current Price: â‚¹{self.price_data['summary_metrics']['current_price']:.2f}\n",
        "            YTD Return: {self.price_data['summary_metrics']['ytd_return']:.2f}%\n",
        "            1-Year Return: {self.price_data['summary_metrics']['return_1y']:.2f}%\n",
        "            3-Year Annualized Return: {self.price_data['summary_metrics']['return_3y']:.2f}%\n",
        "            5-Year Annualized Return: {self.price_data['summary_metrics']['return_5y']:.2f}%\n",
        "            Current RSI: {self.price_data['summary_metrics']['current_rsi']:.2f}\n",
        "            Current Volatility (20-day): {self.price_data['summary_metrics']['current_volatility']:.2f}%\n",
        "            \"\"\"\n",
        "        weighting_instructions = \"\"\n",
        "        if weights:\n",
        "            weights_json = json.dumps(weights, indent=2)\n",
        "            weighting_instructions = f\"\"\"\n",
        "            Apply the following MANUAL WEIGHTS to each domain:\n",
        "            {weights_json}\n",
        "            \"\"\"\n",
        "        else:\n",
        "            weighting_instructions = \"\"\"\n",
        "            Apply adaptive and temporal weighting based on recency and importance factors.\n",
        "            1. More recent data weighted more heavily.\n",
        "            2. Consider dominance_factor and confidence from each sub-agent.\n",
        "            3. For long-term projections, stable fundamentals matter more.\n",
        "            \"\"\"\n",
        "        synthesis_prompt = f\"\"\"\n",
        "        Given the following sub-agent outputs for {self.ticker}:\n",
        "        {outputs_json}\n",
        "        And the following timestamps indicating recency:\n",
        "        {timestamps_json}\n",
        "        {price_context}\n",
        "        {weighting_instructions}\n",
        "        Generate a final structured JSON with keys:\n",
        "        - short_term_range\n",
        "        - medium_term_volatility\n",
        "        - long_term_growth\n",
        "        - risks\n",
        "        - recommendation\n",
        "        - rationale\n",
        "        - final_research_summary\n",
        "        Include disclaimer at end: \"This outlook is based on publicly available web search results, historical price data, and LLM interpretation. For educational use only, not financial advice.\"\n",
        "        Return ONLY the JSON object.\"\n",
        "        \"\"\"\n",
        "        result = query_llm(synthesis_prompt, tools=[{\"type\": \"web_search\"}])\n",
        "        cache = load_cache()\n",
        "        if self.ticker not in cache:\n",
        "            cache[self.ticker] = {}\n",
        "        cache[self.ticker][\"weights_used\"] = weights or \"dynamic weighting\"\n",
        "        save_cache(cache)\n",
        "        update_cache_entry(self.ticker, \"final_synthesis\", result)\n",
        "        return result\n",
        "\n",
        "    def run(self, tasks, weights=None):\n",
        "        print(f\"\\n--- Starting analysis for {self.ticker} ---\\n\")\n",
        "        ticker_tasks = {name: prompt.replace(\"[TICKER]\", self.ticker) for name, prompt in tasks.items()}\n",
        "        sub_outputs = self.run_sequence(ticker_tasks)\n",
        "        final_result = self.final_synthesis(sub_outputs, weights)\n",
        "        print(f\"\\n--- Completed analysis for {self.ticker} ---\\n\")\n",
        "        return {\"sub_agent_outputs\": sub_outputs, \"final_synthesis\": final_result, \"price_data\": self.price_data}"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGQfK6-W4quA"
      },
      "source": [
        "##10. Define Default Sub-Agent Tasks\n",
        "Predefined tasks for Macro, Sector & Company, and Technical analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "default_tasks"
      },
      "execution_count": null,
      "source": [
        "def get_default_tasks():\n",
        "    \"\"\"Get the default tasks for the sub-agents.\"\"\"\n",
        "    return {\n",
        "        \"Macro Analysis\": \"\"\"\n",
        "        Analyze the current Indian macroeconomic factors affecting [TICKER]. Focus on:\n",
        "        1. RBI monetary policies and interest rates\n",
        "        2. Inflation trends in India\n",
        "        3. GDP growth projections\n",
        "        4. Global trade tensions affecting India\n",
        "        5. Any recent government policies affecting the stock\n",
        "\n",
        "        Conduct a web search to find the most recent information available.\n",
        "\n",
        "        Return your analysis as a JSON object with structure:\n",
        "        {\n",
        "            \"summary\": \"...\",\n",
        "            \"risks\": [...],\n",
        "            \"dominance_factor\": \"0-10\",\n",
        "            \"confidence\": \"0-1\",\n",
        "            \"disclaimers\": \"...\"\n",
        "        }\n",
        "        \"\"\",\n",
        "\n",
        "        \"Sector and Company Analysis\": \"\"\"\n",
        "        Analyze sector & company fundamentals for [TICKER]. Focus on:\n",
        "        1. Industry/sector performance\n",
        "        2. Financial health\n",
        "        3. Valuation metrics\n",
        "        4. Recent earnings\n",
        "        5. Management & strategy\n",
        "        6. Competitive positioning\n",
        "\n",
        "        Conduct a web search for the latest info.\n",
        "\n",
        "        Return JSON structured as:\n",
        "        {\n",
        "            \"summary\": \"...\",\n",
        "            \"risks\": [...],\n",
        "            \"dominance_factor\": \"0-10\",\n",
        "            \"confidence\": \"0-1\",\n",
        "            \"disclaimers\": \"...\"\n",
        "        }\n",
        "        \"\"\",\n",
        "\n",
        "        \"Technical Analysis\": \"\"\"\n",
        "        Perform technical analysis for [TICKER]. Focus on:\n",
        "        1. Price action & trends\n",
        "        2. Key indicators (RSI, MACD, etc.)\n",
        "        3. Support & resistance\n",
        "        4. Volume analysis\n",
        "        5. Short-term volatility\n",
        "        6. Chart patterns\n",
        "\n",
        "        Conduct a web search for latest info.\n",
        "\n",
        "        Return JSON structured as:\n",
        "        {\n",
        "            \"summary\": \"...\",\n",
        "            \"risks\": [...],\n",
        "            \"dominance_factor\": \"0-10\",\n",
        "            \"confidence\": \"0-1\",\n",
        "            \"disclaimers\": \"...\"\n",
        "        }\n",
        "        \"\"\"\n",
        "    }"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo77esNM4quB"
      },
      "source": [
        "##11. Stock Outlook Function\n",
        "Convenience wrapper `run_stock_outlook` to manage caching and BaseAgent execution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_stock_outlook"
      },
      "execution_count": null,
      "source": [
        "def run_stock_outlook(\n",
        "    ticker: str,\n",
        "    use_cache: bool = True,\n",
        "    weights: Optional[Dict[str, float]] = None,\n",
        "    start_date=None,\n",
        "    end_date=None\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Returns the final_synthesis for the ticker using cache if available and fresh,\n",
        "    otherwise triggers BaseAgent and refreshes the cache.\n",
        "    \"\"\"\n",
        "    ticker = ticker.upper()\n",
        "    cache = load_cache()\n",
        "\n",
        "    # Ensure a cache entry exists for this ticker\n",
        "    if ticker not in cache:\n",
        "        cache[ticker] = {}\n",
        "\n",
        "    # Price data cache management\n",
        "    if not use_cache or not is_price_cache_fresh(ticker):\n",
        "        try:\n",
        "            print(f\"Fetching fresh price data for {ticker}\")\n",
        "            get_ticker_price_data(ticker, start_date, end_date, force_refresh=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching price data for {ticker}: {e}\")\n",
        "            raise Exception(f\"Failed to retrieve price data for {ticker}. Analysis cannot proceed without price data.\")\n",
        "\n",
        "    # If we have a fresh final_synthesis in cache, return it\n",
        "    if use_cache and ticker in cache and \"final_synthesis\" in cache[ticker]:\n",
        "        entry = cache[ticker][\"final_synthesis\"]\n",
        "        if is_cache_fresh(entry.get(\"last_updated\")):\n",
        "            result = entry.get(\"result\")\n",
        "\n",
        "            # Display the cached plots\n",
        "            try:\n",
        "                price_cache = load_price_cache()\n",
        "                if ticker in price_cache and \"price_history\" in price_cache[ticker]:\n",
        "                    dates = pd.to_datetime(price_cache[ticker][\"price_history\"][\"dates\"])\n",
        "                    stock_data = pd.DataFrame({\n",
        "                        'Open': price_cache[ticker][\"price_history\"][\"open\"],\n",
        "                        'High': price_cache[ticker][\"price_history\"][\"high\"],\n",
        "                        'Low': price_cache[ticker][\"price_history\"][\"low\"],\n",
        "                        'Close': price_cache[ticker][\"price_history\"][\"close\"],\n",
        "                        'Volume': price_cache[ticker][\"price_history\"][\"volume\"],\n",
        "                        'Daily_Return': price_cache[ticker][\"price_history\"][\"indicators\"][\"daily_return\"],\n",
        "                        'Volatility': price_cache[ticker][\"price_history\"][\"indicators\"][\"volatility\"],\n",
        "                        'MA_20': price_cache[ticker][\"price_history\"][\"indicators\"][\"ma_20\"],\n",
        "                        'MA_50': price_cache[ticker][\"price_history\"][\"indicators\"][\"ma_50\"],\n",
        "                        'MA_200': price_cache[ticker][\"price_history\"][\"indicators\"][\"ma_200\"],\n",
        "                        'RSI': price_cache[ticker][\"price_history\"][\"indicators\"][\"rsi\"],\n",
        "                        'MACD': price_cache[ticker][\"price_history\"][\"indicators\"][\"macd\"],\n",
        "                        'Upper_Band': price_cache[ticker][\"price_history\"][\"indicators\"][\"upper_band\"],\n",
        "                        'Lower_Band': price_cache[ticker][\"price_history\"][\"indicators\"][\"lower_band\"]\n",
        "                    }, index=dates)\n",
        "\n",
        "                    nifty_data = yf.download(\"^NSEI\", start=dates[0], end=dates[-1] + timedelta(days=1))\n",
        "                    print(f\"Displaying charts for {ticker} based on cached data\")\n",
        "                    plot_price_charts(stock_data, ticker, nifty_data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error displaying charts from cached data: {e}\")\n",
        "\n",
        "            return cache\n",
        "\n",
        "    # No fresh cache â€” generate result using BaseAgent\n",
        "    print(f\"No fresh cache found for {ticker}, running BaseAgent...\")\n",
        "\n",
        "    # Initialize agent (which will also fetch price data)\n",
        "    agent = BaseAgent(ticker)\n",
        "    tasks = get_default_tasks()\n",
        "    # Pass weights to the run method if provided\n",
        "    full_result = agent.run(tasks, weights)\n",
        "    # Store in cache\n",
        "    print(f'full result : {full_result}')\n",
        "    cache[ticker][\"final_synthesis\"] = {\n",
        "        \"result\": full_result[\"final_synthesis\"],\n",
        "        \"last_updated\": datetime.utcnow().isoformat()\n",
        "    }\n",
        "    print(f'Micro analysis data : {full_result[\"sub_agent_outputs\"][\"Macro Analysis\"]}')\n",
        "    cache[ticker][\"macro_analysis\"] = {\n",
        "        \"result\": full_result[\"sub_agent_outputs\"][\"Macro Analysis\"],\n",
        "        \"last_updated\": datetime.utcnow().isoformat()\n",
        "    }\n",
        "    cache[ticker][\"technical_analysis\"] = {\n",
        "        \"result\": full_result[\"sub_agent_outputs\"][\"Technical Analysis\"],\n",
        "        \"last_updated\": datetime.utcnow().isoformat()\n",
        "    }\n",
        "    cache[ticker][\"sector_and_company_analysis\"] = {\n",
        "        \"result\": full_result[\"sub_agent_outputs\"][\"Sector and Company Analysis\"],\n",
        "        \"last_updated\": datetime.utcnow().isoformat()\n",
        "    }\n",
        "    cache[ticker][\"ticker_last_updated\"] = datetime.utcnow().isoformat()\n",
        "    print(f\"cache data : \", cache[ticker])\n",
        "    save_cache(cache)\n",
        "\n",
        "    # Plot the newly fetched data\n",
        "    try:\n",
        "        price_cache = load_price_cache()\n",
        "        if ticker in price_cache and \"price_history\" in price_cache[ticker]:\n",
        "            dates = pd.to_datetime(price_cache[ticker][\"price_history\"][\"dates\"])\n",
        "            stock_data = pd.DataFrame({\n",
        "                'Open': price_cache[ticker][\"price_history\"][\"open\"],\n",
        "                'High': price_cache[ticker][\"price_history\"][\"high\"],\n",
        "                'Low': price_cache[ticker][\"price_history\"][\"low\"],\n",
        "                'Close': price_cache[ticker][\"price_history\"][\"close\"],\n",
        "                'Volume': price_cache[ticker][\"price_history\"][\"volume\"],\n",
        "                'Daily_Return': price_cache[ticker][\"price_history\"][\"indicators\"][\"daily_return\"],\n",
        "                'Volatility': price_cache[ticker][\"price_history\"][\"indicators\"][\"volatility\"],\n",
        "                'MA_20': price_cache[ticker][\"price_history\"][\"indicators\"][\"ma_20\"],\n",
        "                'MA_50': price_cache[ticker][\"price_history\"][\"indicators\"][\"ma_50\"],\n",
        "                'MA_200': price_cache[ticker][\"price_history\"][\"indicators\"][\"ma_200\"],\n",
        "                'RSI': price_cache[ticker][\"price_history\"][\"indicators\"][\"rsi\"],\n",
        "                'MACD': price_cache[ticker][\"price_history\"][\"indicators\"][\"macd\"],\n",
        "                'Upper_Band': price_cache[ticker][\"price_history\"][\"indicators\"][\"upper_band\"],\n",
        "                'Lower_Band': price_cache[ticker][\"price_history\"][\"indicators\"][\"lower_band\"]\n",
        "            }, index=dates)\n",
        "\n",
        "            nifty_data = yf.download(\"^NSEI\", start=dates[0], end=dates[-1] + timedelta(days=1))\n",
        "            print(f\"Displaying charts for {ticker}\")\n",
        "            plot_price_charts(stock_data, ticker, nifty_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error displaying charts: {e}\")\n",
        "\n",
        "    return cache\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp225i7d4quB"
      },
      "source": [
        "## 12. Similarity & Evaluation Functions\n",
        "Functions to log runs, compute and log similarities, flag runs, and summarize evaluation logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "similarity_evaluation"
      },
      "execution_count": null,
      "source": [
        "def log_run(ticker, prompt_id, agent, output_text, recommendation):\n",
        "    \"\"\"Log a run in the evaluation log.\"\"\"\n",
        "    log_path = \"evaluation_log.csv\"\n",
        "    run_id = hashlib.sha1((ticker + prompt_id + agent + output_text + str(time.time())).encode()).hexdigest()[:10]\n",
        "    entry = {\n",
        "        \"run_id\": run_id,\n",
        "        \"ticker\": ticker,\n",
        "        \"prompt_id\": prompt_id,\n",
        "        \"agent\": agent,\n",
        "        \"recommendation\": recommendation,\n",
        "        \"output_text\": output_text,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "    df_entry = pd.DataFrame([entry])\n",
        "    try:\n",
        "        df_existing = pd.read_csv(log_path)\n",
        "        df_existing = pd.concat([df_existing, df_entry], ignore_index=True)\n",
        "    except FileNotFoundError:\n",
        "        df_existing = df_entry\n",
        "    df_existing.to_csv(log_path, index=False)\n",
        "    print(f\"Logged run: {run_id}\")\n",
        "    return run_id\n",
        "\n",
        "def log_similarity_pairwise(ticker, field, run_id_1, run_id_2, similarity, threshold=0.9):\n",
        "    \"\"\"Log similarity between a pair of runs.\"\"\"\n",
        "    log_path = \"similarity_log.csv\"\n",
        "    entry = {\n",
        "        \"ticker\": ticker,\n",
        "        \"field\": field,\n",
        "        \"run_id_1\": run_id_1,\n",
        "        \"run_id_2\": run_id_2,\n",
        "        \"similarity\": round(similarity, 4),\n",
        "        \"threshold\": threshold,\n",
        "        \"below_threshold\": similarity < threshold\n",
        "    }\n",
        "    df_entry = pd.DataFrame([entry])\n",
        "    try:\n",
        "        df_existing = pd.read_csv(log_path)\n",
        "        df_existing = pd.concat([df_existing, df_entry], ignore_index=True)\n",
        "    except FileNotFoundError:\n",
        "        df_existing = df_entry\n",
        "    df_existing.to_csv(log_path, index=False)\n",
        "\n",
        "def flag_run(run_id: str, flag: str):\n",
        "    \"\"\"Flag a run in evaluation_log.csv.\"\"\"\n",
        "    log_path = \"evaluation_log.csv\"\n",
        "    try:\n",
        "        df = pd.read_csv(log_path)\n",
        "        df.loc[df[\"run_id\"] == run_id, \"flags\"] = flag\n",
        "        df.to_csv(log_path, index=False)\n",
        "        print(f\"âš ï¸ Flagged run {run_id} with flag: {flag}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to flag run: {e}\")\n",
        "\n",
        "def compare_and_log_pairwise(ticker: str, outputs: List[dict], run_ids: List[str], fields: List[str], threshold: float = 0.9) -> Tuple[Dict[str, List[float]], bool]:\n",
        "    \"\"\"Compare and log similarity between multiple outputs.\"\"\"\n",
        "    similarities = {field: [] for field in fields}\n",
        "    below_threshold = False\n",
        "    for field in fields:\n",
        "        for (i, j) in combinations(range(len(outputs)), 2):\n",
        "            val1 = str(outputs[i].get(field, \"\"))\n",
        "            val2 = str(outputs[j].get(field, \"\"))\n",
        "            if val1 and val2:\n",
        "                sim = average_cosine_similarity(val1, val2)\n",
        "                similarities[field].append(sim)\n",
        "                log_similarity_pairwise(\n",
        "                    ticker=ticker,\n",
        "                    field=field,\n",
        "                    run_id_1=run_ids[i],\n",
        "                    run_id_2=run_ids[j],\n",
        "                    similarity=sim,\n",
        "                    threshold=threshold\n",
        "                )\n",
        "                if sim < threshold:\n",
        "                    below_threshold = True\n",
        "                    print(f\"âš ï¸ Flagged pair: {run_ids[i]} vs {run_ids[j]} | Field: {field} | Similarity: {sim:.4f} < {threshold}\")\n",
        "    return similarities, below_threshold\n",
        "\n",
        "def predict(\n",
        "    stock_name: str,\n",
        "    n: int = 1,\n",
        "    check_similarity: bool = False,\n",
        "    similarity_threshold: float = 0.90,\n",
        "    compare_fields: Optional[List[str]] = None,\n",
        "    use_cache: bool = True,\n",
        "    weights: Optional[Dict[str, float]] = None,\n",
        "    start_date=None,\n",
        "    end_date=None\n",
        ") -> Tuple[str, str, str]:\n",
        "    \"\"\"\n",
        "    Run stock analysis with optional similarity checking.\n",
        "    Returns: (results_markdown, evaluation_log_path, similarity_log_path)\n",
        "    \"\"\"\n",
        "    if check_similarity:\n",
        "        if n <= 1:\n",
        "            return (\"âŒ Error: Similarity check requires number of runs (n) > 1.\", None, None)\n",
        "        if use_cache:\n",
        "            return (\"âŒ Error: Please disable 'Use Cache if Available' when running similarity check.\", None, None)\n",
        "    else:\n",
        "        if n > 1:\n",
        "            return (\"âŒ Error: When similarity check is disabled, Number of Runs (n) should be 1.\", None, None)\n",
        "    stock_name = stock_name.upper()\n",
        "    outputs, run_ids = [], []\n",
        "    for i in range(n):\n",
        "        prompt_id = f\"predict_v2_run{i+1}\"\n",
        "        result = run_stock_outlook(\n",
        "            stock_name,\n",
        "            use_cache=use_cache,\n",
        "            weights=weights,\n",
        "            start_date=start_date,\n",
        "            end_date=end_date\n",
        "        )\n",
        "        if isinstance(result, dict) and 'result' in result:\n",
        "            result = result['result']\n",
        "        if isinstance(result, str) and result.strip().startswith('```json'):\n",
        "            result = json.loads(result.strip().replace('```json', '').replace('```','').strip())\n",
        "        outputs.append(result)\n",
        "        text = result.get('final_research_summary', json.dumps(result))\n",
        "        rec = result.get('recommendation', 'UNKNOWN')\n",
        "        run_id = log_run(\n",
        "            ticker=stock_name,\n",
        "            prompt_id=prompt_id,\n",
        "            agent=\"Final Synthesis\",\n",
        "            output_text=text,\n",
        "            recommendation=rec\n",
        "        )\n",
        "        run_ids.append(run_id)\n",
        "    similarity_log = \"\"\n",
        "    if check_similarity and n > 1:\n",
        "        compare_fields = compare_fields or [\"final_research_summary\"]\n",
        "        similarities, below_threshold = compare_and_log_pairwise(\n",
        "            ticker=stock_name,\n",
        "            outputs=outputs,\n",
        "            run_ids=run_ids,\n",
        "            fields=compare_fields,\n",
        "            threshold=similarity_threshold\n",
        "        )\n",
        "        similarity_log += \"\\n\\nðŸ” **Field-Level Similarities**:\\n\"\n",
        "        for field, sims in similarities.items():\n",
        "            if sims:\n",
        "                avg_sim = np.mean(sims)\n",
        "                flag = \"âš ï¸ Below threshold!\" if avg_sim < similarity_threshold else \"âœ…\"\n",
        "                similarity_log += f\"- `{field}`: **{avg_sim:.4f}** {flag}\\n\"\n",
        "            else:\n",
        "                similarity_log += f\"- `{field}`: No valid pairs.\\n\"\n",
        "    final = outputs[-1]\n",
        "    display_parts = []\n",
        "    if 'short_term_range' in final:\n",
        "        display_parts.append(f\"**Short-Term Price Range**: {final['short_term_range']}\")\n",
        "    if 'medium_term_volatility' in final:\n",
        "        display_parts.append(f\"**Medium-Term Volatility**: {final['medium_term_volatility']}\")\n",
        "    if 'long_term_growth' in final:\n",
        "        display_parts.append(f\"**Long-Term Growth Estimate**: {final['long_term_growth']}\")\n",
        "    if 'recommendation' in final:\n",
        "        display_parts.append(f\"**Recommendation**: `{final['recommendation']}`\")\n",
        "    if 'rationale' in final:\n",
        "        display_parts.append(f\"**Rationale**: {final['rationale']}\")\n",
        "    if 'final_research_summary' in final:\n",
        "        display_parts.append(f\"**Summary**:\\n{final['final_research_summary']}\")\n",
        "    if weights:\n",
        "        display_parts.append(f\"**Manual Weights Applied**: {json.dumps(weights, indent=2)}\")\n",
        "    return \"\\n\\n\".join(display_parts) + similarity_log, \"evaluation_log.csv\", \"similarity_log.csv\"\n",
        "\n",
        "# def predict(\n",
        "#     stock_name: str,\n",
        "#     n: int = 1,\n",
        "#     check_similarity: bool = False,\n",
        "#     similarity_threshold: float = 0.90,\n",
        "#     compare_fields: Optional[List[str]] = None,\n",
        "#     use_cache: bool = True,\n",
        "#     weights: Optional[Dict[str, float]] = None,\n",
        "#     start_date=None,\n",
        "#     end_date=None\n",
        "# ) -> Tuple[str, str, str]:\n",
        "#     \"\"\"\n",
        "#     Run stock analysis with optional similarity checking.\n",
        "#     Returns: (results_markdown, evaluation_log_path, similarity_log_path)\n",
        "#     \"\"\"\n",
        "#     if check_similarity:\n",
        "#         if n <= 1:\n",
        "#             return (\"âŒ Error: Similarity check requires number of runs (n) > 1.\", None, None)\n",
        "#         if use_cache:\n",
        "#             return (\"âŒ Error: Please disable 'Use Cache if Available' when running similarity check.\", None, None)\n",
        "#     else:\n",
        "#         if n > 1:\n",
        "#             return (\"âŒ Error: When similarity check is disabled, Number of Runs (n) should be 1.\", None, None)\n",
        "\n",
        "#     stock_name = stock_name.upper()\n",
        "#     outputs, run_ids = [], []\n",
        "\n",
        "#     for i in range(n):\n",
        "#         prompt_id = f\"predict_v2_run{i+1}\"\n",
        "#         result = run_stock_outlook(\n",
        "#             stock_name,\n",
        "#             use_cache=use_cache,\n",
        "#             weights=weights,\n",
        "#             start_date=start_date,\n",
        "#             end_date=end_date\n",
        "#         )\n",
        "#         if isinstance(result, dict) and 'result' in result:\n",
        "#             result = result['result']\n",
        "#         if isinstance(result, str) and result.strip().startswith('```json'):\n",
        "#             result = json.loads(result.strip().replace('```json', '').replace('```', '').strip())\n",
        "\n",
        "#         outputs.append(result)\n",
        "#         text = result.get('final_research_summary', json.dumps(result))\n",
        "#         rec = result.get('recommendation', 'UNKNOWN')\n",
        "#         run_id = log_run(\n",
        "#             ticker=stock_name,\n",
        "#             prompt_id=prompt_id,\n",
        "#             agent=\"Final Synthesis\",\n",
        "#             output_text=text,\n",
        "#             recommendation=rec\n",
        "#         )\n",
        "#         run_ids.append(run_id)\n",
        "\n",
        "#     similarity_log = \"\"\n",
        "#     if check_similarity and n > 1:\n",
        "#         compare_fields = compare_fields or [\"final_research_summary\"]\n",
        "#         similarities, below_threshold = compare_and_log_pairwise(\n",
        "#             ticker=stock_name,\n",
        "#             outputs=outputs,\n",
        "#             run_ids=run_ids,\n",
        "#             fields=compare_fields,\n",
        "#             threshold=similarity_threshold\n",
        "#         )\n",
        "#         similarity_log += \"\\n\\nðŸ” **Field-Level Similarities**:\\n\"\n",
        "#         for field, sims in similarities.items():\n",
        "#             if sims:\n",
        "#                 avg_sim = np.mean(sims)\n",
        "#                 flag = \"âš ï¸ Below threshold!\" if avg_sim < similarity_threshold else \"âœ…\"\n",
        "#                 similarity_log += f\"- `{field}`: **{avg_sim:.4f}** {flag}\\n\"\n",
        "#             else:\n",
        "#                 similarity_log += f\"- `{field}`: No valid pairs.\\n\"\n",
        "\n",
        "#     # Build display using extract_bolded_sections\n",
        "#     final = outputs[-1]\n",
        "#     summary_text = final.get('final_research_summary', '')\n",
        "#     sections = extract_bolded_sections(summary_text if isinstance(summary_text, str) else json.dumps(summary_text))\n",
        "\n",
        "#     display_parts = []\n",
        "#     for key, value in sections.items():\n",
        "#         display_parts.append(f\"**{key}**: {value}\")\n",
        "\n",
        "#     if weights:\n",
        "#         display_parts.append(f\"**Manual Weights Applied**: {json.dumps(weights, indent=2)}\")\n",
        "\n",
        "#     return \"\\n\\n\".join(display_parts) + similarity_log, \"evaluation_log.csv\", \"similarity_log.csv\"\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH9TI_BO4quC"
      },
      "source": [
        "## 13. predict_tamper for Testing Similarity\n",
        "A helper function to deliberately tamper with outputs for testing similarity detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "predict_tamper"
      },
      "execution_count": null,
      "source": [
        "def predict_tamper(\n",
        "    stock_name: str,\n",
        "    n: int = 1,\n",
        "    check_similarity: bool = False,\n",
        "    similarity_threshold: float = 0.90,\n",
        "    compare_fields: Optional[List[str]] = None,\n",
        "    use_cache: bool = True\n",
        "):\n",
        "    \"\"\"Test function that deliberately modifies outputs to test similarity detection.\"\"\"\n",
        "    if check_similarity:\n",
        "        if n <= 1:\n",
        "            return (\"âŒ Error: Similarity check requires n > 1.\", None, None)\n",
        "        if use_cache:\n",
        "            return (\"âŒ Error: Disable cache for similarity check.\", None, None)\n",
        "    else:\n",
        "        if n > 1:\n",
        "            return (\"âŒ Error: When similarity check is disabled, n should be 1.\", None, None)\n",
        "    stock_name = stock_name.upper()\n",
        "    outputs, run_ids = [], []\n",
        "    for i in range(n):\n",
        "        prompt_id = f\"predict_v2_run{i+1}\"\n",
        "        result = run_stock_outlook(stock_name, use_cache=use_cache)\n",
        "        if isinstance(result, dict) and 'result' in result:\n",
        "            result = result['result']\n",
        "        if isinstance(result, str) and result.strip().startswith('```json'):\n",
        "            result = json.loads(result.strip().replace('```json','').replace('```','').strip())\n",
        "        if n > 1 and i == 0:\n",
        "            tampered = result.copy()\n",
        "            if 'final_research_summary' in tampered:\n",
        "                tampered['final_research_summary'] += \" [Simulated update for testing similarity logic.]\"\n",
        "            if 'short_term_range' in tampered:\n",
        "                tampered['short_term_range'] = \"â‚¹999 - â‚¹1,001\"\n",
        "            if 'long_term_growth' in tampered:\n",
        "                tampered['long_term_growth'] = \"0â€“1%\"\n",
        "            result = tampered\n",
        "        outputs.append(result)\n",
        "        text = result.get('final_research_summary', json.dumps(result))\n",
        "        rec = result.get('recommendation', 'UNKNOWN')\n",
        "        run_id = log_run(\n",
        "            ticker=stock_name,\n",
        "            prompt_id=prompt_id,\n",
        "            agent=\"Final Synthesis\",\n",
        "            output_text=text,\n",
        "            recommendation=rec\n",
        "        )\n",
        "        run_ids.append(run_id)\n",
        "    if n > 1:\n",
        "        print(\"\\nðŸ” Similarity Test between first and second run\")\n",
        "    for field in (compare_fields or ['final_research_summary']):\n",
        "        print(f\"\\nðŸ§© Field: {field}\")\n",
        "        print(\"Run 1:\", outputs[0].get(field, '')[:200])\n",
        "        print(\"Run 2:\", outputs[1].get(field, '')[:200])\n",
        "    similarity_log = \"\"\n",
        "    if check_similarity and n > 1:\n",
        "        similarities, below = compare_and_log_pairwise(\n",
        "            ticker=stock_name,\n",
        "            outputs=outputs,\n",
        "            run_ids=run_ids,\n",
        "            fields=compare_fields or ['final_research_summary'],\n",
        "            threshold=similarity_threshold\n",
        "        )\n",
        "        similarity_log += \"\\n\\nðŸ” **Field-Level Similarities**:\\n\"\n",
        "        for field, sims in similarities.items():\n",
        "            if sims:\n",
        "                avg_sim = np.mean(sims)\n",
        "                flag = \"âš ï¸ Below threshold!\" if avg_sim < similarity_threshold else \"âœ…\"\n",
        "                similarity_log += f\"- `{field}`: **{avg_sim:.4f}** {flag}\\n\"\n",
        "            else:\n",
        "                similarity_log += f\"- `{field}`: No valid pairs.\\n\"\n",
        "    final = outputs[-1]\n",
        "    parts = []\n",
        "    if 'short_term_range' in final:\n",
        "        parts.append(f\"**Short-Term Price Range**: {final['short_term_range']}\")\n",
        "    if 'medium_term_volatility' in final:\n",
        "        parts.append(f\"**Medium-Term Volatility**: {final['medium_term_volatility']}\")\n",
        "    if 'long_term_growth' in final:\n",
        "        parts.append(f\"**Long-Term Growth Estimate**: {final['long_term_growth']}\")\n",
        "    if 'recommendation' in final:\n",
        "        parts.append(f\"**Recommendation**: `{final['recommendation']}`\")\n",
        "    if 'final_research_summary' in final:\n",
        "        parts.append(f\"**Summary**:\\n{final['final_research_summary']}\")\n",
        "    return \"\\n\\n\".join(parts) + similarity_log, \"evaluation_log.csv\", \"similarity_log.csv\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3hqo4584quC"
      },
      "source": [
        "## 14. Batch Runner & Log Summaries\n",
        "Functions to run batch predictions, summarize evaluation logs, and initialize/clear logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "batch_and_logs"
      },
      "execution_count": null,
      "source": [
        "def run_batch_predict(\n",
        "    tickers: List[str],\n",
        "    n: int = 3,\n",
        "    check_similarity: bool = True,\n",
        "    similarity_threshold: float = 0.9,\n",
        "    compare_fields: Optional[List[str]] = None,\n",
        "    use_cache: bool = True,\n",
        "    weights: Optional[Dict[str, float]] = None\n",
        "):\n",
        "    \"\"\"Run prediction on multiple tickers as a batch.\"\"\"\n",
        "    for ticker in tickers:\n",
        "        print(f\"\\n======================\")\n",
        "        print(f\"ðŸ“ˆ Processing: {ticker}\")\n",
        "        print(f\"======================\")\n",
        "        try:\n",
        "            md, eval_log, sim_log = predict(\n",
        "                ticker,\n",
        "                n=n,\n",
        "                check_similarity=check_similarity,\n",
        "                similarity_threshold=similarity_threshold,\n",
        "                compare_fields=compare_fields,\n",
        "                use_cache=use_cache,\n",
        "                weights=weights\n",
        "            )\n",
        "            print(md)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed for {ticker}: {e}\")\n",
        "\n",
        "def summarize_evaluation_logs(show_only_flagged=False, last_n_runs=None):\n",
        "    \"\"\"\n",
        "    Summary of past runs from evaluation_log.csv.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(\"evaluation_log.csv\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"âš ï¸ Log file not found.\")\n",
        "        return\n",
        "    if last_n_runs is not None and last_n_runs < len(df):\n",
        "        df = df.tail(last_n_runs)\n",
        "    print(\"ðŸ“Š Summary of Logged Evaluations\\n\")\n",
        "    if \"recommendation\" in df.columns and \"ticker\" in df.columns:\n",
        "        summary = df.groupby([\"ticker\", \"recommendation\"]).size().unstack(fill_value=0)\n",
        "        display(summary)\n",
        "    else:\n",
        "        print(\"âš ï¸ Missing required columns in the log.\")\n",
        "    if \"flags\" in df.columns:\n",
        "        if show_only_flagged:\n",
        "            df = df[df[\"flags\"].notnull() & (df[\"flags\"] != \"\")]\n",
        "            print(\"\\nðŸ“Œ Showing only flagged runs.\\n\")\n",
        "        flagged = df[df[\"flags\"].notnull() & (df[\"flags\"] != \"\")]\n",
        "        if not flagged.empty:\n",
        "            print(\"\\nðŸš© Runs flagged with issues:\")\n",
        "            cols_to_show = [c for c in [\"run_id\", \"ticker\", \"prompt_id\", \"agent\", \"recommendation\", \"flags\"] if c in df.columns]\n",
        "            display(flagged[cols_to_show])\n",
        "            issues = []\n",
        "            for i, row in flagged.iterrows():\n",
        "                for field in str(row[\"flags\"]).split(\",\"):\n",
        "                    issues.append({\"ticker\": row[\"ticker\"], \"field\": field})\n",
        "            if issues:\n",
        "                df_issues = pd.DataFrame(issues)\n",
        "                summary_issues = df_issues.groupby([\"ticker\", \"field\"]).size().reset_index(name=\"low_sim_pairs\")\n",
        "                print(\"\\nðŸ§  Similarity Issues by Ticker & Field:\")\n",
        "                display(summary_issues)\n",
        "        else:\n",
        "            print(\"\\nâœ… No runs flagged with similarity issues.\")\n",
        "\n",
        "def initialize_logs():\n",
        "    \"\"\"Initialize log files with headers if they don't exist.\"\"\"\n",
        "    if not os.path.exists(\"evaluation_log.csv\"):\n",
        "        pd.DataFrame(columns=[\"run_id\", \"ticker\", \"prompt_id\", \"agent\", \"recommendation\", \"output_text\", \"timestamp\"]).to_csv(\"evaluation_log.csv\", index=False)\n",
        "    if not os.path.exists(\"similarity_log.csv\"):\n",
        "        pd.DataFrame(columns=[\"ticker\", \"field\", \"run_id_1\", \"run_id_2\", \"similarity\", \"below_threshold\", \"threshold\"]).to_csv(\"similarity_log.csv\", index=False)\n",
        "\n",
        "def clear_cache():\n",
        "    \"\"\"Clear the entire LLM result cache.\"\"\"\n",
        "    if pathlib.Path(CACHE_FILE_PATH).exists():\n",
        "        os.remove(CACHE_FILE_PATH)\n",
        "        print(\"LLM cache cleared successfully.\")\n",
        "    else:\n",
        "        print(\"No LLM cache file found.\")\n",
        "\n",
        "def clear_price_cache():\n",
        "    \"\"\"Clear the entire price data cache.\"\"\"\n",
        "    if pathlib.Path(PRICE_CACHE_PATH).exists():\n",
        "        os.remove(PRICE_CACHE_PATH)\n",
        "        print(\"Price cache cleared successfully.\")\n",
        "    else:\n",
        "        print(\"No price cache file found.\")\n",
        "\n",
        "def clear_ticker_cache(ticker):\n",
        "    \"\"\"Clear the cache for a specific ticker.\"\"\"\n",
        "    ticker = ticker.upper()\n",
        "    cache = load_cache()\n",
        "    if ticker in cache:\n",
        "        del cache[ticker]\n",
        "        save_cache(cache)\n",
        "        print(f\"LLM cache for {ticker} cleared successfully.\")\n",
        "    else:\n",
        "        print(f\"No LLM cache found for {ticker}.\")\n",
        "    price_cache = load_price_cache()\n",
        "    if ticker in price_cache:\n",
        "        del price_cache[ticker]\n",
        "        save_price_cache(price_cache)\n",
        "        print(f\"Price cache for {ticker} cleared successfully.\")\n",
        "    else:\n",
        "        print(f\"No price cache found for {ticker}.\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su1q3GJa4quD"
      },
      "source": [
        "## 15. Example Usage\n",
        "Initialize logs, run predictions on sample tickers, and demonstrate cache clearing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "example_usage"
      },
      "execution_count": null,
      "source": [
        "# # Initialize log files\n",
        "# initialize_logs()\n",
        "\n",
        "# # Example: Simple prediction with default settings\n",
        "# result, eval_log, sim_log = predict(\"TCS.NS\", n=1, check_similarity=False, use_cache=True)\n",
        "# print(\"\\n=== RELIANCE.NS Analysis ===\")\n",
        "# print(result)\n",
        "\n",
        "# # Example: Prediction with manual weights\n",
        "# custom_weights = {\n",
        "#     \"Macro Analysis\": 0.3,\n",
        "#     \"Sector and Company Analysis\": 0.5,\n",
        "#     \"Technical Analysis\": 0.2\n",
        "# }\n",
        "# result, eval_log, sim_log = predict(\n",
        "#     \"TCS.NS\",\n",
        "#     n=1,\n",
        "#     check_similarity=False,\n",
        "#     use_cache=True,\n",
        "#     weights=custom_weights\n",
        "# )\n",
        "# print(\"\\n=== TCS.NS Analysis with Custom Weights ===\")\n",
        "# print(result)\n",
        "\n",
        "# # Clear cache for a specific ticker (uncomment to use)\n",
        "# # clear_ticker_cache(\"RELIANCE.NS\")\n",
        "\n",
        "# # Batch run (uncomment)\n",
        "# # run_batch_predict([\"RELIANCE.NS\", \"TCS.NS\", \"INFY.NS\"], n=1, check_similarity=False, use_cache=True)\n",
        "\n",
        "# # Similarity check example (uncomment)\n",
        "# # result, eval_log, sim_log = predict(\n",
        "# #     \"RELIANCE.NS\",\n",
        "# #     n=3,\n",
        "# #     check_similarity=True,\n",
        "# #     similarity_threshold=0.90,\n",
        "# #     compare_fields=[\"short_term_range\", \"long_term_growth\", \"final_research_summary\"],\n",
        "# #     use_cache=False\n",
        "# # )\n",
        "# # print(\"\\n=== RELIANCE.NS Similarity Check ===\")\n",
        "# # print(result)\n",
        "\n",
        "# # Example: Display summary of evaluation logs (uncomment)\n",
        "# # summarize_evaluation_logs(last_n_runs=10)\n",
        "\n",
        "# print(\"\\nStock Screener with Price Data Integration is ready for use.\")\n",
        "# print(\"Use the predict() function to analyze a stock with parameters:\")\n",
        "# print(\"  - stock_name: e.g., 'INFY.NS'\")\n",
        "# print(\"  - n: Number of runs for similarity checking\")\n",
        "# print(\"  - check_similarity: True/False\")\n",
        "# print(\"  - use_cache: True/False\")\n",
        "# print(\"  - weights: Optional manual weights dict\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXK-m0U34quD"
      },
      "source": [
        "## 16. Gradio UI Integration\n",
        "A light Gradio-based front-end for interactive stock analysis in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to original view"
      ],
      "metadata": {
        "id": "SVc9nJ0HMecK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import gradio as gr\n",
        "# from typing import Optional, List\n",
        "# from IPython.display import display, Markdown\n",
        "\n",
        "# # â”€â”€â”€ Monkey-patch to restore missing Component alias for newer Gradio versions â”€â”€â”€\n",
        "# from gradio.components.base import Component as _BaseComponent\n",
        "# import gradio.components as components\n",
        "# components.Component = _BaseComponent\n",
        "\n",
        "# # â”€â”€â”€ UTF-8 sanitizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# def sanitize_for_utf8(text: str) -> str:\n",
        "#     return text.encode('utf-8', errors='replace').decode('utf-8')\n",
        "\n",
        "# # â”€â”€â”€ Gradio wrapper around our existing predict() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# def gradio_predict(\n",
        "#     stock_ticker: str,\n",
        "#     n_runs: int,\n",
        "#     check_similarity: bool,\n",
        "#     similarity_threshold: float,\n",
        "#     compare_fields: List[str],\n",
        "#     use_cache: bool,\n",
        "#     show_similarity_summary: bool\n",
        "# ):\n",
        "#     try:\n",
        "#         # Ensure our logs exist\n",
        "#         initialize_logs()\n",
        "\n",
        "#         # Run the core predict() function\n",
        "#         prediction_md, eval_log, sim_log = predict(\n",
        "#             stock_name=stock_ticker.strip().upper(),\n",
        "#             n=n_runs,\n",
        "#             check_similarity=check_similarity,\n",
        "#             similarity_threshold=similarity_threshold,\n",
        "#             compare_fields=compare_fields,\n",
        "#             use_cache=use_cache\n",
        "#         )\n",
        "\n",
        "#         # Clean up any weird characters\n",
        "#         prediction_md = sanitize_for_utf8(prediction_md)\n",
        "\n",
        "#         # Optionally strip out the similarity section\n",
        "#         if not show_similarity_summary:\n",
        "#             prediction_md = prediction_md.split(\"ðŸ” **Field-Level Similarities**\")[0].strip()\n",
        "\n",
        "#         return prediction_md, eval_log, sim_log\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return f\"âŒ Error: {sanitize_for_utf8(str(e))}\", None, None\n",
        "\n",
        "# # â”€â”€â”€ Dynamic field updater â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# def update_fields(similarity_enabled: bool):\n",
        "#     if similarity_enabled:\n",
        "#         return gr.update(value=[\n",
        "#             \"short_term_range\",\n",
        "#             \"long_term_growth\",\n",
        "#             \"final_research_summary\"\n",
        "#         ])\n",
        "#     return gr.update(value=[])\n",
        "\n",
        "# # â”€â”€â”€ Build the Gradio UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# with gr.Blocks(title=\"ðŸ“‰ Stock Recommendation Evaluator\") as demo:\n",
        "#     gr.Markdown(\"Run multi-pass stock outlook generation and evaluate consistency using LLM outputs.\")\n",
        "\n",
        "#     with gr.Row():\n",
        "#         stock_input = gr.Textbox(label=\"ðŸ“ˆ Stock Ticker\", placeholder=\"e.g., RELIANCE.NS\")\n",
        "#         n_runs      = gr.Slider(1, 5, value=1, step=1, label=\"ðŸ” Number of Runs (n)\")\n",
        "\n",
        "#     with gr.Row():\n",
        "#         check_similarity     = gr.Checkbox(label=\"ðŸ§ª Check Similarity Between Runs\")\n",
        "#         similarity_threshold = gr.Slider(0.70, 1.1, value=0.92, step=0.01, label=\"âš ï¸ Similarity Threshold\")\n",
        "\n",
        "#     fields_to_compare = gr.CheckboxGroup(\n",
        "#         choices=[\"short_term_range\", \"long_term_growth\", \"final_research_summary\"],\n",
        "#         label=\"ðŸ§© Fields to Compare for Similarity\",\n",
        "#         value=[]\n",
        "#     )\n",
        "\n",
        "#     with gr.Row():\n",
        "#         use_cache       = gr.Checkbox(value=True,  label=\"ðŸ“¦ Use Cache if Available\")\n",
        "#         show_sim_summary= gr.Checkbox(value=False, label=\"ðŸ” Show Similarity Summary\")\n",
        "\n",
        "#     submit_btn       = gr.Button(\"ðŸš€ Run Prediction\")\n",
        "\n",
        "#     output_md        = gr.Markdown(label=\"ðŸ“Š Prediction + Similarity Summary\")\n",
        "#     eval_log_file    = gr.File(label=\"â¬‡ï¸ Evaluation Log\")\n",
        "#     sim_log_file     = gr.File(label=\"ðŸ§  Similarity Log\")\n",
        "\n",
        "#     # Wire up the button and dynamic field toggling\n",
        "#     submit_btn.click(\n",
        "#         fn=gradio_predict,\n",
        "#         inputs=[\n",
        "#             stock_input,\n",
        "#             n_runs,\n",
        "#             check_similarity,\n",
        "#             similarity_threshold,\n",
        "#             fields_to_compare,\n",
        "#             use_cache,\n",
        "#             show_sim_summary\n",
        "#         ],\n",
        "#         outputs=[\n",
        "#             output_md,\n",
        "#             eval_log_file,\n",
        "#             sim_log_file\n",
        "#         ]\n",
        "#     )\n",
        "#     check_similarity.change(\n",
        "#         fn=update_fields,\n",
        "#         inputs=check_similarity,\n",
        "#         outputs=fields_to_compare\n",
        "#     )\n",
        "\n",
        "# # Launch the interface\n",
        "# demo.launch(share=True, debug=False)\n"
      ],
      "metadata": {
        "id": "9IAAZVvYF0IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5LLjp-UF0KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s4ZBO-4UF0MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kSYyP4WrF0OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ft1LbYvF0Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3vIvVVSlF0TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main vs Evaluation tabs separated"
      ],
      "metadata": {
        "id": "N2DtEVfOMYe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Display the cached plots\n",
        "def display_plots_from_price_cache(ticker):\n",
        "            ticker = ticker.upper()\n",
        "            try:\n",
        "                price_cache = load_price_cache()\n",
        "                if ticker in price_cache and \"price_history\" in price_cache[ticker]:\n",
        "                    dates = pd.to_datetime(price_cache[ticker][\"price_history\"][\"dates\"])\n",
        "                    stock_data = pd.DataFrame({\n",
        "                        'Open': price_cache[ticker][\"price_history\"][\"open\"],\n",
        "                        'High': price_cache[ticker][\"price_history\"][\"high\"],\n",
        "                        'Low': price_cache[ticker][\"price_history\"][\"low\"],\n",
        "                        'Close': price_cache[ticker][\"price_history\"][\"close\"],\n",
        "                        'Volume': price_cache[ticker][\"price_history\"][\"volume\"],\n",
        "                        'Daily_Return': price_cache[ticker][\"price_history\"][\"indicators\"][\"daily_return\"],\n",
        "                        'Volatility': price_cache[ticker][\"price_history\"][\"indicators\"][\"volatility\"],\n",
        "                        'MA_20': price_cache[ticker][\"price_history\"][\"indicators\"][\"ma_20\"],\n",
        "                        'MA_50': price_cache[ticker][\"price_history\"][\"indicators\"][\"ma_50\"],\n",
        "                        'MA_200': price_cache[ticker][\"price_history\"][\"indicators\"][\"ma_200\"],\n",
        "                        'RSI': price_cache[ticker][\"price_history\"][\"indicators\"][\"rsi\"],\n",
        "                        'MACD': price_cache[ticker][\"price_history\"][\"indicators\"][\"macd\"],\n",
        "                        'Upper_Band': price_cache[ticker][\"price_history\"][\"indicators\"][\"upper_band\"],\n",
        "                        'Lower_Band': price_cache[ticker][\"price_history\"][\"indicators\"][\"lower_band\"]\n",
        "                    }, index=dates)\n",
        "\n",
        "                    nifty_data = yf.download(\"^NSEI\", start=dates[0], end=dates[-1] + timedelta(days=1))\n",
        "                    # print(f\"Displaying charts for {ticker} based on cached data\")\n",
        "                    plots = plot_price_charts(stock_data, ticker, nifty_data)\n",
        "                    # return plots[\"price_chart\"], plots[\"bollinger_chart\"], plots[\"rsi_chart\"], plots[\"volatility_chart\"]\n",
        "                    return {\n",
        "                            \"price_chart\": plots[\"price_chart\"],\n",
        "                            \"bollinger_chart\": plots[\"bollinger_chart\"],\n",
        "                            \"rsi_chart\": plots[\"rsi_chart\"],\n",
        "                            \"volatility_chart\": plots[\"volatility_chart\"]\n",
        "                        }\n",
        "            except Exception as e:\n",
        "                print(f\"Error displaying charts from cached data: {e}\")\n",
        "                return None, None, None, None, None\n",
        "# display_plots_from_price_cache(\"TCS.NS\")"
      ],
      "metadata": {
        "id": "li8CS9qO0nZk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "# --------------- Helper Functions ---------------\n",
        "\n",
        "# def load_json(file_path):\n",
        "#     \"\"\"Load JSON file dynamically.\"\"\"\n",
        "#     with open(file_path, 'r') as f:\n",
        "#         data = json.load(f)\n",
        "#     return data\n",
        "\n",
        "def clean_json_string(raw_string):\n",
        "    \"\"\"Clean markdown artifacts and parse JSON if possible.\"\"\"\n",
        "    if isinstance(raw_string, dict):\n",
        "        return raw_string  # Already a dictionary, no need to clean\n",
        "    if not raw_string:\n",
        "        return {}\n",
        "    cleaned = raw_string.replace('```json', '').replace('```', '').strip()\n",
        "    if not cleaned:\n",
        "        return {}\n",
        "    try:\n",
        "        # print(f'Cleaned data : {cleaned}')\n",
        "        return json.loads(cleaned)\n",
        "    except Exception:\n",
        "        # print(f\"JSON parse failed. Attempting to extract bolded sections. {Exception}\")\n",
        "        return extract_bolded_sections(raw_string)\n",
        "\n",
        "def extract_bolded_sections(content):\n",
        "    \"\"\"Extract **bolded** sections if JSON parse fails.\"\"\"\n",
        "    result = {}\n",
        "    pattern = r'\\*\\*(.*?)\\*\\*:?\\s*([\\s\\S]*?)(?=(\\n\\s*\\*\\*|$))'\n",
        "    matches = re.findall(pattern, content)\n",
        "    # print(f'Matches : {matches}')\n",
        "    for key, value, _ in matches:\n",
        "        result[key.strip()] = value.strip()\n",
        "    return result\n",
        "\n",
        "# def get_analysis_content(analysis_data):\n",
        "#     \"\"\"Return analysis content dynamically.\"\"\"\n",
        "#     # print(f'stock data : {stock_data}')\n",
        "#     raw_result = analysis_data.get('result', '')\n",
        "#     last_updated = analysis_data.get('last_updated', '')\n",
        "#     parsed = clean_json_string(raw_result)\n",
        "#     content = \"\"\n",
        "#     print(f'content : {content}')\n",
        "#     if last_updated:\n",
        "#         content += f\"**Last Updated**: {last_updated}\\n\\n\"\n",
        "#     if isinstance(parsed, dict):\n",
        "#         for key, value in parsed.items():\n",
        "#             if isinstance(value, dict):\n",
        "#                 # Format key-value pairs from dicts\n",
        "#                 content += f\"**{key.replace('_', ' ').title()}**:\\n\"\n",
        "#                 for sub_key, sub_val in value.items():\n",
        "#                     content += f\"- {sub_key.title()}: {sub_val}\\n\"\n",
        "#                 content += \"\\n\"\n",
        "#             elif isinstance(value, list):\n",
        "#                 # Format list values as bullet points\n",
        "#                 content += f\"**{key.replace('_', ' ').title()}**:\\n\"\n",
        "#                 for item in value:\n",
        "#                     content += f\"- {item}\\n\"\n",
        "#                 content += \"\\n\"\n",
        "#             else:\n",
        "#                 # Format regular key-value\n",
        "#                 content += f\"**{key.replace('_', ' ').title()}**: {value}\\n\\n\"\n",
        "#     else:\n",
        "#         content += f\"**{raw_result}**\\n\\n\"\n",
        "\n",
        "#     print(f'Content : {content}')\n",
        "#     return content if content.strip() else \"**No data available.**\"\n",
        "\n",
        "\n",
        "# --------------- Main Gradio App ---------------\n",
        "\n",
        "# Load the data once\n",
        "# file_path = '/content/cache.json'\n",
        "# stock_data = load_json(file_path)\n",
        "\n",
        "# # Extract list of tickers\n",
        "# tickers = list(stock_data.keys())\n",
        "analysis_tabs = [\n",
        "    'macro_analysis',\n",
        "    'sector_and_company_analysis',\n",
        "    'technical_analysis',\n",
        "    'final_synthesis'\n",
        "]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VaXEAGXW1Lz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json_from_string(text):\n",
        "    \"\"\"Extract JSON object from a string, if present.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return None\n",
        "    json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "    if json_match:\n",
        "        try:\n",
        "            return json.loads(json_match.group())\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def get_analysis_content(analysis_data):\n",
        "    \"\"\"Return analysis content dynamically with embedded JSON handled.\"\"\"\n",
        "    raw_result = analysis_data.get('result', '')\n",
        "    last_updated = analysis_data.get('last_updated', '')\n",
        "    content = \"\"\n",
        "\n",
        "    # Start with last updated\n",
        "    if last_updated:\n",
        "        content += f\"**Last Updated**: {last_updated}\\n\\n\"\n",
        "\n",
        "    parsed = {}\n",
        "    if isinstance(raw_result, str):\n",
        "        embedded_json = extract_json_from_string(raw_result)\n",
        "        if embedded_json:\n",
        "            parsed.update(embedded_json)\n",
        "            pre_text = raw_result.split('{')[0].strip()\n",
        "            if pre_text:\n",
        "                parsed[\"description\"] = pre_text\n",
        "        else:\n",
        "            parsed[\"description\"] = raw_result.strip()\n",
        "\n",
        "    elif isinstance(raw_result, dict):\n",
        "        parsed.update(raw_result)\n",
        "        for key, value in list(parsed.items()):\n",
        "            if isinstance(value, str):\n",
        "                embedded_json = extract_json_from_string(value)\n",
        "                if embedded_json:\n",
        "                    del parsed[key]  # remove the old string value\n",
        "                    parsed.update(embedded_json)  # merge extracted JSON\n",
        "\n",
        "    # Now format everything in parsed\n",
        "    for key, value in parsed.items():\n",
        "        title = key.replace('_', ' ').title()\n",
        "        if isinstance(value, list):\n",
        "            content += f\"**{title}**:\\n\"\n",
        "            for item in value:\n",
        "                content += f\"- {item}\\n\"\n",
        "            content += \"\\n\"\n",
        "        elif isinstance(value, dict):\n",
        "            content += f\"**{title}**:\\n\"\n",
        "            for sub_key, sub_val in value.items():\n",
        "                content += f\"- {sub_key.title()}: {sub_val}\\n\"\n",
        "            content += \"\\n\"\n",
        "        else:\n",
        "            content += f\"**{title}**: {value}\\n\\n\"\n",
        "\n",
        "    return content.strip() if content.strip() else \"**No data available.**\"\n"
      ],
      "metadata": {
        "id": "njyTrnyhKEtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gradio_ui"
      },
      "execution_count": null,
      "source": [
        "# import gradio as gr\n",
        "# # â”€â”€â”€ Monkeyâ€patch to restore the missing Component alias â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# from gradio.components.base import Component as _BaseComponent\n",
        "# import gradio.components as components\n",
        "# components.Component = _BaseComponent\n",
        "\n",
        "# # â”€â”€â”€ Helper functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# def _sanitize_utf8(text: str) -> str:\n",
        "#     return text.encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\")\n",
        "\n",
        "# def _update_fields(similarity_enabled: bool):\n",
        "#     defaults = [\"short_term_range\", \"long_term_growth\", \"final_research_summary\"]\n",
        "#     return gr.update(value=defaults if similarity_enabled else [])\n",
        "\n",
        "# def update_output(stock_results, ticker, analysis_type):\n",
        "#     return get_analysis_content(stock_results, ticker, analysis_type)\n",
        "\n",
        "# def set_selected_tab(evt: gr.SelectData):\n",
        "#     return evt.value  # evt.value gives the tab id (analysis_type)\n",
        "\n",
        "# def update_all_tabs(results, ticker):\n",
        "#   outputs = []\n",
        "#   for analysis_type in analysis_tabs:\n",
        "#       outputs.append(update_output(results, ticker, analysis_type))\n",
        "#   return outputs\n",
        "\n",
        "# def gradio_predict(\n",
        "#     stock_ticker: str,\n",
        "#     n_runs: int,\n",
        "#     check_similarity: bool,\n",
        "#     similarity_threshold: float,\n",
        "#     compare_fields: list,\n",
        "#     use_cache: bool,\n",
        "#     show_similarity_summary: bool\n",
        "# ):\n",
        "#     \"\"\"Thin wrapper around predict() to drive the UI.\"\"\"\n",
        "#     initialize_logs()\n",
        "    # md, eval_log_path, sim_log_path = predict(\n",
        "    #     stock_name=stock_ticker.upper().strip(),\n",
        "    #     n=n_runs,\n",
        "    #     check_similarity=check_similarity,\n",
        "    #     similarity_threshold=similarity_threshold,\n",
        "    #     compare_fields=compare_fields or [],\n",
        "    #     use_cache=use_cache\n",
        "    # )\n",
        "#     if not show_similarity_summary:\n",
        "#         md = md.split(\"ðŸ” **Field-Level Similarities**\")[0].rstrip()\n",
        "#     # update_output(md, stock_ticker, analysis_type)\n",
        "#     stock_results = update_all_tabs(md, stock_ticker)\n",
        "#     return stock_results, eval_log_path, sim_log_path\n",
        "\n",
        "# # â”€â”€â”€ Build the Gradio App â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# with gr.Blocks(title=\"ðŸ“‰ Stock Recommendation Evaluator\") as demo:\n",
        "#     with gr.Tabs():\n",
        "#         # â”€ Prediction Tab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#         with gr.TabItem(\"Prediction\"):\n",
        "#             stock_input = gr.Textbox(\n",
        "#                 label=\"ðŸ“ˆ Stock Ticker\",\n",
        "#                 placeholder=\"e.g. RELIANCE.NS\"\n",
        "#             )\n",
        "#             run_btn = gr.Button(\"ðŸš€ Run Prediction\")\n",
        "#             # out_md = gr.Markdown(label=\"ðŸ“Š Prediction Summary\")\n",
        "\n",
        "#             # ticker_input = gr.Textbox(label=\"Enter Ticker\", placeholder=\"e.g., RELIANCE.NS\")\n",
        "#             # pred_btn = gr.Button(\"Get summary\")\n",
        "#             selected_tab = gr.State(value=\"macro_analysis\")  # default tab\n",
        "#             tab_outputs = []\n",
        "#             with gr.Tabs(selected=\"macro_analysis\") as tabs:\n",
        "#               for analysis_type in analysis_tabs:\n",
        "#                   with gr.Tab(label=analysis_type.replace('_', ' ').title(), id=analysis_type):\n",
        "#                       output = gr.Markdown()\n",
        "#                       tab_outputs.append(output)\n",
        "\n",
        "\n",
        "\n",
        "#               # Track selected tab\n",
        "#               tabs.select(fn=set_selected_tab, inputs=None, outputs=selected_tab)\n",
        "\n",
        "#               # pred_btn.click(\n",
        "#               #     fn=update_all_tabs,\n",
        "#               #     inputs=[stock_input],\n",
        "#               #     outputs=tab_outputs  # temp placeholder\n",
        "#               # )\n",
        "\n",
        "#         # â”€ Evaluation Tab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#         with gr.TabItem(\"Evaluation\"):\n",
        "#             n_runs = gr.Slider(\n",
        "#                 minimum=1, maximum=5, step=1, value=1,\n",
        "#                 label=\"ðŸ” Number of Runs\"\n",
        "#             )\n",
        "#             use_cache = gr.Checkbox(\n",
        "#                 label=\"ðŸ“¦ Use Cache if Available\",\n",
        "#                 value=True\n",
        "#             )\n",
        "#             show_sim_summary = gr.Checkbox(\n",
        "#                 label=\"ðŸ” Show Similarity Summary\",\n",
        "#                 value=False\n",
        "#             )\n",
        "#             check_similarity = gr.Checkbox(label=\"ðŸ§ª Check Similarity Between Runs\")\n",
        "#             similarity_threshold = gr.Slider(\n",
        "#                 minimum=0.70, maximum=1.0, step=0.01, value=0.90,\n",
        "#                 label=\"âš ï¸ Similarity Threshold\"\n",
        "#             )\n",
        "#             fields_to_compare = gr.CheckboxGroup(\n",
        "#                 choices=[\"short_term_range\", \"long_term_growth\", \"final_research_summary\"],\n",
        "#                 value=[\"short_term_range\", \"final_research_summary\"],\n",
        "#                 label=\"ðŸ§© Fields to Compare for Similarity\"\n",
        "#             )\n",
        "#             eval_file = gr.File(label=\"â¬‡ï¸ Evaluation Log\")\n",
        "#             sim_file  = gr.File(label=\"ðŸ§  Similarity Log\")\n",
        "\n",
        "#     # â”€â”€â”€ Wire up callbacks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#     run_btn.click(\n",
        "#         fn=gradio_predict,\n",
        "#         inputs=[\n",
        "#             stock_input,\n",
        "#             n_runs,\n",
        "#             check_similarity,\n",
        "#             similarity_threshold,\n",
        "#             fields_to_compare,\n",
        "#             use_cache,\n",
        "#             show_sim_summary\n",
        "#         ],\n",
        "#         outputs=[*tab_outputs, eval_file, sim_file]\n",
        "#     )\n",
        "#     check_similarity.change(\n",
        "#         fn=_update_fields,\n",
        "#         inputs=check_similarity,\n",
        "#         outputs=fields_to_compare\n",
        "#     )\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     demo.launch(share=True, debug=False)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple tabs for various analyses"
      ],
      "metadata": {
        "id": "tYA_5ctCMPXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_2(\n",
        "    stock_name,\n",
        "    analysis_type=\"final_synthesis\",\n",
        "    n=1,\n",
        "    check_similarity=False,\n",
        "    similarity_threshold=0.9,\n",
        "    compare_fields=False,\n",
        "    use_cache=True,\n",
        "    show_similarity_summary=False,\n",
        "    weights=None,\n",
        "    start_date=None,\n",
        "    end_date=None\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Run stock analysis with optional similarity checking.\n",
        "    Returns: (results_markdown, evaluation_log_path, similarity_log_path)\n",
        "    \"\"\"\n",
        "    if check_similarity:\n",
        "        if n <= 1:\n",
        "            print(\"âŒ Error: Similarity check requires number of runs (n) > 1.\", None, None)\n",
        "        if use_cache:\n",
        "            print(\"âŒ Error: Please disable 'Use Cache if Available' when running similarity check.\", None, None)\n",
        "    else:\n",
        "        if n > 1:\n",
        "            print(\"âŒ Error: When similarity check is disabled, Number of Runs (n) should be 1.\", None, None)\n",
        "\n",
        "    stock_name = stock_name.upper()\n",
        "    outputs, run_ids = [], []\n",
        "\n",
        "    for i in range(n):\n",
        "        prompt_id = f\"predict_v2_run{i+1}\"\n",
        "        # print(\"Weights : \", weights)\n",
        "        result = run_stock_outlook(\n",
        "            stock_name,\n",
        "            use_cache=use_cache,\n",
        "            weights=weights,\n",
        "            start_date=start_date,\n",
        "            end_date=end_date\n",
        "        )\n",
        "        try:\n",
        "          result[stock_name][analysis_type][\"result\"] = clean_json_string(result[stock_name][analysis_type][\"result\"])\n",
        "        except:\n",
        "          return \"No data found\"\n",
        "        outputs.append(result)\n",
        "        text = result[stock_name][analysis_type][\"result\"].get('final_research_summary')\n",
        "        rec = result[stock_name][analysis_type][\"result\"].get('recommendation', 'UNKNOWN')\n",
        "        run_id = log_run(\n",
        "            ticker=stock_name,\n",
        "            prompt_id=prompt_id,\n",
        "            agent=\"Final Synthesis\",\n",
        "            output_text=text,\n",
        "            recommendation=rec\n",
        "        )\n",
        "        run_ids.append(run_id)\n",
        "\n",
        "    similarity_log = \"\"\n",
        "    if check_similarity and n > 1:\n",
        "        compare_fields = compare_fields or [\"final_research_summary\"]\n",
        "        similarities, below_threshold = compare_and_log_pairwise(\n",
        "            ticker=stock_name,\n",
        "            outputs=outputs,\n",
        "            run_ids=run_ids,\n",
        "            fields=compare_fields,\n",
        "            threshold=similarity_threshold\n",
        "        )\n",
        "        similarity_log += \"Field-Level Similarities:\"\n",
        "        for field, sims in similarities.items():\n",
        "            if sims:\n",
        "                avg_sim = np.mean(sims)\n",
        "                flag = \"Below threshold!\" if avg_sim < similarity_threshold else \" \"\n",
        "                similarity_log += f\"- `{field}`: {avg_sim:.4f}  {flag}\\n\"\n",
        "            else:\n",
        "                similarity_log += f\"- `{field}`: No valid pairs.\\n\"\n",
        "\n",
        "    return outputs, \"evaluation_log.csv\", \"similarity_log.csv\"\n",
        "\n"
      ],
      "metadata": {
        "id": "hbQ7SvXOHQQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict_2(\"TCS.NS\")"
      ],
      "metadata": {
        "id": "76CBqBzDPsD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_all_tabs_2(results, ticker):\n",
        "  outputs = []\n",
        "  for analysis_type in analysis_tabs:\n",
        "      outputs.append(get_analysis_content(results[ticker][analysis_type]))\n",
        "  return outputs\n",
        "\n",
        "# def update_output(sub_agent_data):\n",
        "#     # print(f'update_output : {results}')\n",
        "#     return get_analysis_content(sub_agent_data)\n",
        "\n",
        "def set_selected_tab(evt: gr.SelectData):\n",
        "    return evt.value  # evt.value gives the tab id (analysis_type)"
      ],
      "metadata": {
        "id": "sJmjCO8U4o2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "# â”€â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "FIELDS_TO_COMPARE = [\"short_term_range\", \"long_term_growth\", \"final_research_summary\"]\n",
        "\n",
        "# â”€â”€â”€ Monkey Patch for Compatibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if not hasattr(gr.components.Component, \"get_block_name\"):\n",
        "    gr.components.Component.get_block_name = lambda self: None\n",
        "\n",
        "def _update_fields(sim_enabled: bool):\n",
        "    defaults = [\"short_term_range\",\"long_term_growth\",\"final_research_summary\"]\n",
        "    return gr.update(value=defaults if sim_enabled else [])\n",
        "\n",
        "# def get_analysis_content(stock_results, ticker, analysis_type):\n",
        "#     if stock_results and ticker in stock_results:\n",
        "#         return stock_results[ticker].get(analysis_type, \"No data available.\")\n",
        "#     return \"No data available.\"\n",
        "\n",
        "def update_output(stock_results, ticker, analysis_type):\n",
        "    return gr.Markdown.update(value=get_analysis_content(stock_results, ticker, analysis_type))\n",
        "\n",
        "def gradio_predict(stock_ticker, analysis_type, n_runs, check_similarity, similarity_threshold,\n",
        "                   compare_fields, use_cache, show_similarity_summary, add_weights,\n",
        "                   macro_weight_val, sector_weight_val, tech_weight_val\n",
        "                   ):\n",
        "    try:\n",
        "      initialize_logs()\n",
        "      # print(stock_ticker, analysis_type, n_runs, check_similarity, similarity_threshold,\n",
        "      #              compare_fields, use_cache, show_similarity_summary, add_weights,\n",
        "      #              macro_weight_val, sector_weight_val, tech_weight_val)\n",
        "      if add_weights == False:\n",
        "          custom_weights = None\n",
        "      else:\n",
        "          custom_weights = {\n",
        "                \"Macro Analysis\": float(macro_weight_val),\n",
        "                \"Sector and Company Analysis\": float(sector_weight_val),\n",
        "                \"Technical Analysis\": float(tech_weight_val)\n",
        "            }\n",
        "      md, eval_log_path, sim_log_path = predict_2(\n",
        "          stock_name=stock_ticker.upper().strip(),\n",
        "          n=n_runs,\n",
        "          check_similarity=check_similarity,\n",
        "          similarity_threshold=similarity_threshold,\n",
        "          compare_fields=compare_fields or [],\n",
        "          use_cache=use_cache,\n",
        "          weights=custom_weights\n",
        "      )\n",
        "      # update_all_tabs_2(md, stock_ticker)\n",
        "      # if not show_similarity_summary:\n",
        "      #     md = md.split(\"Field-Level Similarities\")[0].rstrip()\n",
        "\n",
        "      stock_results = update_all_tabs_2(md[0], stock_ticker )\n",
        "      plots = display_plots_from_price_cache(stock_ticker)\n",
        "      # print(f'stocks tabs : {stock_results}')  # Should be equal to len(tab_outputs)\n",
        "      # print(f'type : {type(plots[\"price_chart\"])}')  # Should be go.Figure\n",
        "      return (*stock_results, eval_log_path, sim_log_path, plots[\"price_chart\"], plots[\"bollinger_chart\"], plots[\"rsi_chart\"], plots[\"volatility_chart\"])\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âš ï¸ Error: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        # return {tab: error_msg for tab in analysis_tabs}, None, None\n",
        "        return [error_msg] + [None, None, None, None, None, None]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HDlQNs3SyrE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_re = gradio_predict(stock_ticker=\"MAHSEAMLES.NS\", analysis_type=\"final_synthesis\", n_runs=1, check_similarity=False, similarity_threshold=0.9,\n",
        "                         compare_fields=False, use_cache=True, show_similarity_summary=False, add_weights=False,\n",
        "                         macro_weight_val=0.2, sector_weight_val=0.3, tech_weight_val=0.3 )\n",
        "predict_re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi3Vty5D76-F",
        "outputId": "87e7df58-b0d0-444c-e732-4923b1746515",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying charts for MAHSEAMLES.NS based on cached data\n",
            "Logged run: b093afbfa2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"**Last Updated**: 2025-05-03T12:54:45.332716\\n\\n**Summary**: Maharashtra Seamless Limited is navigating a complex macroeconomic environment characterized by supportive monetary policies and declining inflation, juxtaposed with global trade tensions and potential import tariffs.\\n\\n**Risks**:\\n- Potential reduction in GDP growth due to U.S. tariffs.\\n- Increased competition from imported steel affecting domestic market share.\\n\\n**Dominance Factor**: 6\\n\\n**Confidence**: 0.8\\n\\n**Disclaimers**: This analysis is based on information available as of May 3, 2025, and is subject to change with evolving economic conditions.\\n\\n**Description**: The performance of Maharashtra Seamless Limited (MAHSEAMLES.NS) is influenced by several Indian macroeconomic factors:\\n\\n1. **RBI Monetary Policies and Interest Rates**: In February 2025, the Reserve Bank of India (RBI) reduced the policy repo rate by 25 basis points to 6.25%, aiming to stimulate economic growth.  Additionally, the RBI plans to purchase bonds worth â‚¹1.25 trillion in May 2025 to inject liquidity into the banking system, effectively acting as a rate cut. \\n\\n2. **Inflation Trends in India**: As of March 2025, India's inflation rate stood at 3.34%, a decrease from 5.69% in December 2023.  This downward trend may enhance consumer purchasing power and reduce input costs for industries, including steel manufacturing.\\n\\n3. **GDP Growth Projections**: The RBI projects a GDP growth rate of 6.7% for the fiscal year 2025-26.  However, the Ministry of Finance warns that escalating trade tensions and geopolitical uncertainties could pose significant risks to this outlook. \\n\\n4. **Global Trade Tensions Affecting India**: The U.S. has imposed 26% tariffs on certain imports, potentially reducing India's GDP growth by 20-40 basis points in FY 2025-26.  This could impact export-oriented sectors, including steel.\\n\\n5. **Recent Government Policies Affecting the Stock**: In response to increased steel imports, particularly from China, India is considering imposing a temporary tax of 15%-25% on Chinese steel imports to protect domestic producers.  This measure could benefit local steel manufacturers like Maharashtra Seamless Limited by reducing competition from imported steel.\\n\\nGiven these factors, Maharashtra Seamless Limited may experience both opportunities and challenges. While supportive monetary policies and declining inflation are favorable, global trade tensions and potential import tariffs introduce uncertainties.\\n\\n```json\",\n",
              " \"**Last Updated**: 2025-05-03T12:54:45.332726\\n\\n**Summary**: Maharashtra Seamless Limited operates in a dynamic Indian steel industry characterized by robust demand growth, increased import competition, and a push towards greener manufacturing processes.\\n\\n**Risks**:\\n- Margin pressures due to rising imports and input costs.\\n- Regulatory challenges related to environmental compliance.\\n\\n**Dominance Factor**: 6\\n\\n**Confidence**: 0.7\\n\\n**Disclaimers**: Specific financial and strategic information for Maharashtra Seamless Limited is not available in the provided sources. The analysis is based on industry-wide data.\\n\\n**Description**: Maharashtra Seamless Limited (MAHSEAMLES.NS) operates within India's steel industry, which is currently experiencing significant developments.\\n\\n**1. Industry/Sector Performance:**\\nIndia's steel demand is projected to grow by 8-9% in 2025, driven by increased construction activities in housing and infrastructure sectors, as well as demand from engineering and packaging industries.  However, the industry faces challenges from rising imports, particularly from China, Japan, and Vietnam, which have led to price pressures on domestic mills. \\n\\n**2. Financial Health:**\\nSpecific financial data for Maharashtra Seamless Limited is not available in the provided sources. Generally, Indian steel companies are experiencing margin pressures due to increased imports and rising input costs, such as coking coal. For instance, coking coal prices rose from $200 per tonne in Q2 FY25 to $225 per tonne in December 2024. \\n\\n**3. Valuation Metrics:**\\nNo specific valuation metrics for Maharashtra Seamless Limited are available in the provided sources.\\n\\n**4. Recent Earnings:**\\nRecent earnings data for Maharashtra Seamless Limited is not available in the provided sources.\\n\\n**5. Management & Strategy:**\\nInformation regarding the management and strategic initiatives of Maharashtra Seamless Limited is not available in the provided sources.\\n\\n**6. Competitive Positioning:**\\nThe Indian steel industry is facing increased competition from imports, particularly from China, which has led to price pressures on domestic producers. The government has introduced a 12% provisional safeguard tariff on select steel imports to protect local producers.  Additionally, the industry is under pressure to transition to greener manufacturing processes in response to global efforts to reduce greenhouse gas emissions. \\n\\n```json\",\n",
              " \"**Last Updated**: 2025-05-03T12:54:45.332721\\n\\n**Summary**: Maharashtra Seamless Limited is exhibiting signs of potential recovery, with strong buying pressure indicated by the MFI. However, the stock remains below its long-term moving averages, suggesting that a definitive uptrend has yet to be established.\\n\\n**Risks**:\\n- Continued downward trend if resistance levels are not breached.\\n- Potential market volatility affecting stock performance.\\n\\n**Dominance Factor**: 6\\n\\n**Confidence**: 0.7\\n\\n**Disclaimers**: This analysis is based on data available as of May 3, 2025, and is subject to change with market conditions.\\n\\n**Description**: As of May 3, 2025, Maharashtra Seamless Limited (MAHSEAMLES.NS) is trading at â‚¹647.60. The stock has experienced a year-to-date decline of 14.82% and a one-year decline of 27.27%.\\n\\n**1. Price Action & Trends:**\\nThe stock has been in a downtrend over the past year, with a 27.27% decline. However, recent data indicates a potential reversal, as the stock has risen from â‚¹601.20 on March 3, 2025, to â‚¹647.60 currently.\\n\\n**2. Key Indicators:**\\n- **Relative Strength Index (RSI):** Currently at 46.76, suggesting the stock is approaching neutral territory.\\n- **Moving Averages:**\\n  - **20-day EMA:** â‚¹625.86 (Neutral)\\n  - **50-day EMA:** â‚¹645.13 (Neutral)\\n  - **100-day EMA:** â‚¹657.08 (Neutral)\\n  - **200-day EMA:** â‚¹675.15 (Neutral)\\n- **Money Flow Index (MFI):** At 72.84, indicating strong buying pressure.\\n\\n**3. Support & Resistance:**\\n- **Support Levels:** â‚¹551.97, â‚¹575.08, â‚¹610.77\\n- **Resistance Levels:** â‚¹669.57, â‚¹692.68, â‚¹728.37\\n\\n**4. Volume Analysis:**\\nRecent trading volumes have been moderate, with no significant spikes indicating strong buying or selling pressure.\\n\\n**5. Short-term Volatility:**\\nThe stock's current volatility is 3.51%, suggesting moderate price fluctuations in the short term.\\n\\n**6. Chart Patterns:**\\nNo specific chart patterns are evident from the available data.\\n\\nIn summary, Maharashtra Seamless Limited is showing signs of potential recovery, with key indicators like the MFI suggesting strong buying pressure. However, the stock remains below its long-term moving averages, indicating that a definitive uptrend has yet to be established.\\n\\n```json\",\n",
              " '**Last Updated**: 2025-05-03T12:54:45.332677\\n\\n**Short Term Range**:\\n- Support: 551.97\\n- Resistance: 669.57\\n\\n**Medium Term Volatility**: Moderate, with a current volatility of 3.51%\\n\\n**Long Term Growth**: Stable fundamentals with potential growth due to supportive monetary policies and declining inflation, but challenged by global trade tensions and import competition.\\n\\n**Risks**:\\n- Potential reduction in GDP growth due to U.S. tariffs.\\n- Increased competition from imported steel affecting domestic market share.\\n- Margin pressures due to rising imports and input costs.\\n- Regulatory challenges related to environmental compliance.\\n- Continued downward trend if resistance levels are not breached.\\n- Potential market volatility affecting stock performance.\\n\\n**Recommendation**: Hold\\n\\n**Rationale**: Maharashtra Seamless Limited is showing signs of potential recovery with strong buying pressure, but remains below long-term moving averages. Supportive macroeconomic factors are counterbalanced by risks from global trade tensions and import competition.\\n\\n**Final Research Summary**: Maharashtra Seamless Limited is navigating a complex environment with supportive monetary policies and declining inflation juxtaposed with global trade tensions and potential import tariffs. The stock shows potential recovery signs, but remains below long-term moving averages, indicating caution.\\n\\n**Disclaimer**: This outlook is based on publicly available web search results, historical price data, and LLM interpretation. For educational use only, not financial advice.',\n",
              " 'evaluation_log.csv',\n",
              " 'similarity_log.csv',\n",
              " Figure({\n",
              "     'data': [{'mode': 'lines',\n",
              "               'name': 'MAHSEAMLES.NS Close',\n",
              "               'type': 'scatter',\n",
              "               'x': array([datetime.datetime(2022, 5, 4, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 5, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 6, 0, 0), ...,\n",
              "                           datetime.datetime(2025, 4, 29, 0, 0),\n",
              "                           datetime.datetime(2025, 4, 30, 0, 0),\n",
              "                           datetime.datetime(2025, 5, 2, 0, 0)], dtype=object),\n",
              "               'y': array([288.21450806, 293.70083618, 292.2265625 , ..., 676.54998779,\n",
              "                           655.45001221, 647.59997559])},\n",
              "              {'line': {'dash': 'dash'},\n",
              "               'mode': 'lines',\n",
              "               'name': '50-day MA',\n",
              "               'type': 'scatter',\n",
              "               'x': array([datetime.datetime(2022, 5, 4, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 5, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 6, 0, 0), ...,\n",
              "                           datetime.datetime(2025, 4, 29, 0, 0),\n",
              "                           datetime.datetime(2025, 4, 30, 0, 0),\n",
              "                           datetime.datetime(2025, 5, 2, 0, 0)], dtype=object),\n",
              "               'y': array([  0.        ,   0.        ,   0.        , ..., 664.59799927,\n",
              "                           665.44099976, 665.99699951])},\n",
              "              {'line': {'dash': 'dash'},\n",
              "               'mode': 'lines',\n",
              "               'name': '200-day MA',\n",
              "               'type': 'scatter',\n",
              "               'x': array([datetime.datetime(2022, 5, 4, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 5, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 6, 0, 0), ...,\n",
              "                           datetime.datetime(2025, 4, 29, 0, 0),\n",
              "                           datetime.datetime(2025, 4, 30, 0, 0),\n",
              "                           datetime.datetime(2025, 5, 2, 0, 0)], dtype=object),\n",
              "               'y': array([  0.        ,   0.        ,   0.        , ..., 653.30936279,\n",
              "                           653.43778625, 653.50582306])},\n",
              "              {'mode': 'lines',\n",
              "               'name': '^NSEI (scaled)',\n",
              "               'type': 'scatter',\n",
              "               'x': array([datetime.datetime(2022, 5, 4, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 5, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 6, 0, 0), ...,\n",
              "                           datetime.datetime(2025, 4, 29, 0, 0),\n",
              "                           datetime.datetime(2025, 4, 30, 0, 0),\n",
              "                           datetime.datetime(2025, 5, 2, 0, 0)], dtype=object),\n",
              "               'y': array([[288.21450806],\n",
              "                           [288.30179331],\n",
              "                           [283.61157817],\n",
              "                           ...,\n",
              "                           [420.56253876],\n",
              "                           [420.53229608],\n",
              "                           [420.74831525]])}],\n",
              "     'layout': {'template': '...',\n",
              "                'title': {'text': 'MAHSEAMLES.NS Price & MAs'},\n",
              "                'xaxis': {'title': {'text': 'Date'}},\n",
              "                'yaxis': {'title': {'text': 'Price'}}}\n",
              " }),\n",
              " Figure({\n",
              "     'data': [{'mode': 'lines',\n",
              "               'name': 'Close',\n",
              "               'type': 'scatter',\n",
              "               'x': array([datetime.datetime(2022, 5, 4, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 5, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 6, 0, 0), ...,\n",
              "                           datetime.datetime(2025, 4, 29, 0, 0),\n",
              "                           datetime.datetime(2025, 4, 30, 0, 0),\n",
              "                           datetime.datetime(2025, 5, 2, 0, 0)], dtype=object),\n",
              "               'y': array([288.21450806, 293.70083618, 292.2265625 , ..., 676.54998779,\n",
              "                           655.45001221, 647.59997559])},\n",
              "              {'line': {'width': 0.5},\n",
              "               'mode': 'lines',\n",
              "               'name': 'Upper Band',\n",
              "               'type': 'scatter',\n",
              "               'x': array([datetime.datetime(2022, 5, 4, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 5, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 6, 0, 0), ...,\n",
              "                           datetime.datetime(2025, 4, 29, 0, 0),\n",
              "                           datetime.datetime(2025, 4, 30, 0, 0),\n",
              "                           datetime.datetime(2025, 5, 2, 0, 0)], dtype=object),\n",
              "               'y': array([  0.        ,   0.        ,   0.        , ..., 741.41342597,\n",
              "                           741.49908518, 742.40129363])},\n",
              "              {'fill': 'tonexty',\n",
              "               'line': {'width': 0.5},\n",
              "               'mode': 'lines',\n",
              "               'name': 'Lower Band',\n",
              "               'type': 'scatter',\n",
              "               'x': array([datetime.datetime(2022, 5, 4, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 5, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 6, 0, 0), ...,\n",
              "                           datetime.datetime(2025, 4, 29, 0, 0),\n",
              "                           datetime.datetime(2025, 4, 30, 0, 0),\n",
              "                           datetime.datetime(2025, 5, 2, 0, 0)], dtype=object),\n",
              "               'y': array([  0.        ,   0.        ,   0.        , ..., 634.96157403,\n",
              "                           631.2059136 , 626.78870271])}],\n",
              "     'layout': {'template': '...',\n",
              "                'title': {'text': 'MAHSEAMLES.NS Bollinger Bands'},\n",
              "                'xaxis': {'title': {'text': 'Date'}},\n",
              "                'yaxis': {'title': {'text': 'Price'}}}\n",
              " }),\n",
              " Figure({\n",
              "     'data': [{'mode': 'lines',\n",
              "               'name': 'RSI',\n",
              "               'type': 'scatter',\n",
              "               'x': array([datetime.datetime(2022, 5, 4, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 5, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 6, 0, 0), ...,\n",
              "                           datetime.datetime(2025, 4, 29, 0, 0),\n",
              "                           datetime.datetime(2025, 4, 30, 0, 0),\n",
              "                           datetime.datetime(2025, 5, 2, 0, 0)], dtype=object),\n",
              "               'y': array([ 0.        ,  0.        ,  0.        , ..., 45.41929946, 53.11381259,\n",
              "                           46.76366752])}],\n",
              "     'layout': {'annotations': [{'showarrow': False,\n",
              "                                 'text': 'Overbought',\n",
              "                                 'x': 1,\n",
              "                                 'xanchor': 'right',\n",
              "                                 'xref': 'x domain',\n",
              "                                 'y': 70,\n",
              "                                 'yanchor': 'bottom',\n",
              "                                 'yref': 'y'},\n",
              "                                {'showarrow': False,\n",
              "                                 'text': 'Oversold',\n",
              "                                 'x': 1,\n",
              "                                 'xanchor': 'right',\n",
              "                                 'xref': 'x domain',\n",
              "                                 'y': 30,\n",
              "                                 'yanchor': 'bottom',\n",
              "                                 'yref': 'y'}],\n",
              "                'shapes': [{'line': {'dash': 'dash'},\n",
              "                            'type': 'line',\n",
              "                            'x0': 0,\n",
              "                            'x1': 1,\n",
              "                            'xref': 'x domain',\n",
              "                            'y0': 70,\n",
              "                            'y1': 70,\n",
              "                            'yref': 'y'},\n",
              "                           {'line': {'dash': 'dash'},\n",
              "                            'type': 'line',\n",
              "                            'x0': 0,\n",
              "                            'x1': 1,\n",
              "                            'xref': 'x domain',\n",
              "                            'y0': 30,\n",
              "                            'y1': 30,\n",
              "                            'yref': 'y'}],\n",
              "                'template': '...',\n",
              "                'title': {'text': 'MAHSEAMLES.NS RSI'},\n",
              "                'xaxis': {'title': {'text': 'Date'}},\n",
              "                'yaxis': {'range': [0, 100], 'title': {'text': 'RSI'}}}\n",
              " }),\n",
              " Figure({\n",
              "     'data': [{'mode': 'lines',\n",
              "               'name': 'MAHSEAMLES.NS Volatility (%)',\n",
              "               'type': 'scatter',\n",
              "               'x': array([datetime.datetime(2022, 5, 4, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 5, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 6, 0, 0), ...,\n",
              "                           datetime.datetime(2025, 4, 29, 0, 0),\n",
              "                           datetime.datetime(2025, 4, 30, 0, 0),\n",
              "                           datetime.datetime(2025, 5, 2, 0, 0)], dtype=object),\n",
              "               'y': array([0.        , 0.        , 0.        , ..., 3.46450834, 3.51434598,\n",
              "                           3.51177734])},\n",
              "              {'mode': 'lines',\n",
              "               'name': '^NSEI Volatility',\n",
              "               'type': 'scatter',\n",
              "               'x': array([datetime.datetime(2022, 5, 4, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 5, 0, 0),\n",
              "                           datetime.datetime(2022, 5, 6, 0, 0), ...,\n",
              "                           datetime.datetime(2025, 4, 29, 0, 0),\n",
              "                           datetime.datetime(2025, 4, 30, 0, 0),\n",
              "                           datetime.datetime(2025, 5, 2, 0, 0)], dtype=object),\n",
              "               'y': array([       nan,        nan,        nan, ..., 1.34472686, 1.34392295,\n",
              "                           1.33968638])}],\n",
              "     'layout': {'template': '...',\n",
              "                'title': {'text': 'MAHSEAMLES.NS vs ^NSEI Volatility'},\n",
              "                'xaxis': {'title': {'text': 'Date'}},\n",
              "                'yaxis': {'title': {'text': 'Volatility (%)'}}}\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_weights = [round(i * 0.1, 1) for i in range(1, 11)]\n",
        "\n",
        "def set_selected_tab(evt: gr.SelectData):\n",
        "    return evt.value  # evt.value gives the tab id (analysis_type)\n",
        "with gr.Blocks(title=\"ðŸ“‰ Stock Recommendation Evaluator\") as demo:\n",
        "    stock_input = gr.Textbox(label=\"Stock Ticker\", placeholder=\"e.g. RELIANCE.NS\")\n",
        "    add_weights = gr.Checkbox(label=\"Adjust Persona\", value=False)\n",
        "    with gr.Row(visible=False) as weight_row:\n",
        "          macro_weight = gr.Dropdown(\n",
        "                choices=generate_weights,\n",
        "                label=\"Macro Analysis Weight\",\n",
        "                value=0.3\n",
        "            )\n",
        "          sector_weight = gr.Dropdown(\n",
        "                choices=generate_weights,\n",
        "                label=\"Sector & Company Analysis Weight\",\n",
        "                value=0.2\n",
        "            )\n",
        "          tech_weight = gr.Dropdown(\n",
        "                choices=generate_weights,\n",
        "                label=\"Technical Analysis Weight\",\n",
        "                value=0.2\n",
        "            )\n",
        "\n",
        "    run_btn = gr.Button(\"Run Analysis\")\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Technical Charts\"):\n",
        "            # chart_button = gr.Button(\"Show Charts\")\n",
        "            price_plot = gr.Plot(label=\"Price Chart\")\n",
        "            bollinger_plot = gr.Plot(label=\"Bollinger Bands\")\n",
        "            rsi_plot = gr.Plot(label=\"RSI\")\n",
        "            vol_plot = gr.Plot(label=\"Volatility\")\n",
        "\n",
        "        with gr.TabItem(\"Analysis\"):\n",
        "            selected_tab = gr.State(value=\"macro_analysis\")\n",
        "            tab_outputs = []\n",
        "            with gr.Tabs(selected=\"macro_analysis\") as tabs:\n",
        "                for analysis_type in analysis_tabs:\n",
        "                    with gr.Tab(label=analysis_type.replace('_', ' ').title(), id=analysis_type):\n",
        "                        output = gr.Markdown()\n",
        "                        tab_outputs.append(output)\n",
        "\n",
        "                tabs.select(fn=set_selected_tab, inputs=None, outputs=selected_tab)\n",
        "\n",
        "        with gr.TabItem(\"Evaluation\"):\n",
        "            n_runs = gr.Slider(minimum=1, maximum=5, step=1, value=1, label=\"Number of Runs\")\n",
        "            use_cache = gr.Checkbox(label=\"Use Cache if Available\", value=True)\n",
        "            show_sim_summary = gr.Checkbox(label=\"Show Similarity Summary\", value=False)\n",
        "            check_similarity = gr.Checkbox(label=\"Check Similarity Between Runs\")\n",
        "            similarity_threshold = gr.Slider(minimum=0.70, maximum=1.0, step=0.01, value=0.90, label=\"Similarity Threshold\")\n",
        "            fields_to_compare = gr.CheckboxGroup(\n",
        "                choices=[\"short_term_range\", \"long_term_growth\", \"final_research_summary\"],\n",
        "                value=[\"short_term_range\", \"final_research_summary\"],\n",
        "                label=\"Fields to Compare for Similarity\"\n",
        "            )\n",
        "            eval_file = gr.File(label=\"â¬‡ï¸ Evaluation Log\")\n",
        "            sim_file  = gr.File(label=\"ðŸ§  Similarity Log\")\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=gradio_predict,\n",
        "        inputs=[\n",
        "            stock_input,\n",
        "            selected_tab,\n",
        "            n_runs,\n",
        "            check_similarity,\n",
        "            similarity_threshold,\n",
        "            fields_to_compare,\n",
        "            use_cache,\n",
        "            show_sim_summary,\n",
        "            add_weights,\n",
        "            macro_weight,\n",
        "            sector_weight,\n",
        "            tech_weight\n",
        "        ],\n",
        "        outputs=tab_outputs + [eval_file, sim_file, price_plot, bollinger_plot, rsi_plot, vol_plot]\n",
        "    )\n",
        "\n",
        "    # chart_button.click(\n",
        "    #       fn=display_plots_from_price_cache,\n",
        "    #       inputs=[stock_input],\n",
        "    #       outputs=[price_plot, bollinger_plot, rsi_plot, vol_plot]\n",
        "    #   )\n",
        "\n",
        "    def toggle_weight_fields(show):\n",
        "      return gr.update(visible=show)\n",
        "\n",
        "    add_weights.change(\n",
        "        fn=toggle_weight_fields,\n",
        "        inputs=[add_weights],\n",
        "        outputs=[weight_row]\n",
        "    )\n",
        "\n",
        "    check_similarity.change(\n",
        "        fn=_update_fields,\n",
        "        inputs=check_similarity,\n",
        "        outputs=fields_to_compare\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True, debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "ljXPYheWy0W2",
        "outputId": "24cbbbaf-7907-4ab2-89c0-6ce21b5e042d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "IMPORTANT: You are using gradio version 3.38.0, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://3d895d0763f3b2007f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3d895d0763f3b2007f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# results = get_analysis_content(stock_data, \"TCS.NS\", \"sector_and_company_analysis\")"
      ],
      "metadata": {
        "id": "48t0HLeF3PHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Individual Tab UI"
      ],
      "metadata": {
        "id": "SwZCgGZzzA_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import gradio as gr\n",
        "# with gr.Blocks(title=\"Stock Screener Evaluator\") as demo:\n",
        "#     ticker_input = gr.Textbox(label=\"Enter Ticker\", placeholder=\"e.g., RELIANCE.NS\")\n",
        "#     pred_btn = gr.Button(\"Get summary\")\n",
        "#     selected_tab = gr.State(value=\"macro_analysis\")  # default tab\n",
        "#     tab_outputs = []\n",
        "#     with gr.Tabs(selected=\"macro_analysis\") as tabs:\n",
        "#         for analysis_type in analysis_tabs:\n",
        "#             with gr.Tab(label=analysis_type.replace('_', ' ').title(), id=analysis_type):\n",
        "#                 output = gr.Markdown()\n",
        "#                 tab_outputs.append(output)\n",
        "\n",
        "#     # Track selected tab\n",
        "#     tabs.select(fn=set_selected_tab, inputs=None, outputs=selected_tab)\n",
        "\n",
        "#     def update_all_tabs(ticker):\n",
        "#       outputs = []\n",
        "#       for analysis_type in analysis_tabs:\n",
        "#           outputs.append(update_output(ticker, analysis_type))\n",
        "#       return outputs\n",
        "\n",
        "#     pred_btn.click(\n",
        "#         fn=update_all_tabs,\n",
        "#         inputs=[ticker_input],\n",
        "#         outputs=tab_outputs  # temp placeholder\n",
        "#     )\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     demo.launch(share=True, debug=False)\n"
      ],
      "metadata": {
        "id": "6gtPt9SBEogV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gradio as gr\n",
        "\n",
        "# def _sanitize_utf8(text: str) -> str:\n",
        "#     return text.encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\")\n",
        "\n",
        "# def _update_fields(sim_enabled: bool):\n",
        "#     defaults = [\"short_term_range\",\"long_term_growth\",\"final_research_summary\"]\n",
        "#     return gr.update(value=defaults if sim_enabled else [])\n",
        "\n",
        "# def get_ticker_values(ticker):\n",
        "#   try:\n",
        "#     return stock_data[ticker]\n",
        "#   except:\n",
        "#     return {\"error\": \"ticker not found\"}\n",
        "\n",
        "# with gr.Blocks(title=\"ðŸ“‰ Stock Recommendation Evaluator\") as demo:\n",
        "#     stock_input = gr.Textbox(label=\"ðŸ“ˆ Stock Ticker\", placeholder=\"e.g. RELIANCE.NS\")\n",
        "#     run_btn     = gr.Button(fn= get_ticker_values, inputs=[\"text\"], outputs=[\"text\"])\n",
        "#     out_md      = gr.Markdown(label=\"ðŸ“Š Recommendation & Summary\")\n",
        "#     print(out_md)\n",
        "#     with gr.Tabs():\n",
        "#         for i in range(5):\n",
        "#           # â”€â”€â”€â”€â”€â”€â”€ 1) Prediction â”€â”€â”€â”€â”€â”€â”€\n",
        "#           with gr.TabItem(\"Prediction\"):\n",
        "#               stock_input = gr.Textbox(label=\"ðŸ“ˆ Stock Ticker\", placeholder=\"e.g. RELIANCE.NS\")\n",
        "#               run_btn     = gr.Button(\"ðŸš€ Run Prediction\")\n",
        "#               out_md      = gr.Markdown(label=\"ðŸ“Š Recommendation & Summary\")\n",
        "\n",
        "#         # â”€â”€â”€ 2) Price, Volatility & Deep-dives â”€â”€â”€\n",
        "#         with gr.TabItem(\"Price, Volatility & Deep-dives\"):\n",
        "#             price_metrics = gr.Markdown(label=\"ðŸ“ˆ Price Metrics\")\n",
        "#             volatility_gauge = gr.Markdown(label=\"âš–ï¸ Volatility & Growth\")\n",
        "#             subagent_breakdown = gr.JSON(label=\"ðŸ§  Sub-Agent Outputs\", visible=False)\n",
        "\n",
        "#         # â”€â”€â”€ 3) Advanced Risk Analysis & Rationale â”€â”€â”€\n",
        "#         with gr.TabItem(\"Advanced Risk Analysis & Rationale\"):\n",
        "#             risk_dashboard = gr.Markdown(label=\"ðŸš© Risk Dashboard\")\n",
        "#             rationale_md   = gr.Markdown(label=\"ðŸ“ Rationale & Full Summary\")\n",
        "\n",
        "#         # â”€â”€â”€ 4) Evaluation â”€â”€â”€\n",
        "#         with gr.TabItem(\"Evaluation\"):\n",
        "#             n_runs               = gr.Slider(1,5, value=1, step=1, label=\"ðŸ” Number of Runs\")\n",
        "#             use_cache            = gr.Checkbox(value=True, label=\"ðŸ“¦ Use Cache if Available\")\n",
        "#             show_sim_summary     = gr.Checkbox(value=False, label=\"ðŸ” Show Similarity Summary\")\n",
        "#             check_similarity     = gr.Checkbox(label=\"ðŸ§ª Check Similarity Between Runs\")\n",
        "#             similarity_threshold = gr.Slider(0.70,1.0, value=0.90, step=0.01, label=\"âš ï¸ Similarity Threshold\")\n",
        "#             fields_to_compare    = gr.CheckboxGroup(\n",
        "#                 choices=[\"short_term_range\",\"long_term_growth\",\"final_research_summary\"],\n",
        "#                 value=[\"short_term_range\",\"final_research_summary\"],\n",
        "#                 label=\"ðŸ§© Fields to Compare\"\n",
        "#             )\n",
        "#             eval_file            = gr.File(label=\"â¬‡ï¸ Evaluation Log\")\n",
        "#             sim_file             = gr.File(label=\"ðŸ§  Similarity Log\")\n",
        "\n",
        "#     # wire up the click\n",
        "#     run_btn.click(\n",
        "#         fn=gradio_predict,\n",
        "#         inputs=[\n",
        "#             stock_input,\n",
        "#             n_runs,\n",
        "#             check_similarity,\n",
        "#             similarity_threshold,\n",
        "#             fields_to_compare,\n",
        "#             use_cache,\n",
        "#             show_sim_summary\n",
        "#         ],\n",
        "#         outputs=[\n",
        "#             out_md,\n",
        "#             price_metrics,\n",
        "#             volatility_gauge,\n",
        "#             subagent_breakdown,\n",
        "#             risk_dashboard,\n",
        "#             rationale_md,\n",
        "#             eval_file,\n",
        "#             sim_file\n",
        "#         ]\n",
        "#     )\n",
        "\n",
        "#     # dynamically update compare-fields if similarity toggles\n",
        "#     check_similarity.change(\n",
        "#         fn=_update_fields,\n",
        "#         inputs=check_similarity,\n",
        "#         outputs=fields_to_compare\n",
        "#     )\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     demo.launch(share=True, debug=False)\n"
      ],
      "metadata": {
        "id": "igVO7wO19bLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7ze6lEJ9bNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySQVOz_64quE"
      },
      "source": [
        "**Notebook setup complete.**  \n",
        "Save this file as `Stock_Outlook_and_Recommendation_Tool.ipynb` and run each cell in order to initialize, configure caches, define classes and functions, and launch the Gradio interface for interactive use."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uWM36GRb9bQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FrOOXZ-jBQFx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}